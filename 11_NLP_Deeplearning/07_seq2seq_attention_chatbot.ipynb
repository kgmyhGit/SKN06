{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebda4f9-a2f4-4af2-b1fb-36c01c48d8f0",
   "metadata": {},
   "source": [
    "# Attention mechanism \n",
    "\n",
    "- Seq2Seq 모델의 문제점\n",
    "    - Seq2Seq 모델은 Encoder에서 입력 시퀀스에 대한 특성을 **하나의 고정된 context vector**에 압축하여 Decoder로 전달 한다. Decoder는 이 context vector를 이용해서 출력 시퀀스를 만든다.\n",
    "    - 하나의 고정된 크기의 vector에 모든 입력 시퀀스의 정보를 넣다보니 정보 손실이 발생한다.\n",
    "    - Decoder에서 출력 시퀀스를 생성할 때 동일한 context vector를 기반으로 한다. 그러나 각 생성 토큰마다 입력 시퀀스에서 참조해야 할 중요도가 다를 수 있다. seq2seq는 encoder의 마지막 hidden state를 context로 받은 뒤 그것을 이용해 모든 출력 단어들을 생성하므로 그 중요도에 대한 반영이 안된다.\n",
    "\n",
    "## Attention Mechanism 아이디어\n",
    "-  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder의 입력 문장(context vector)을 다시 참고 하자는 것. 이때 전체 입력 문장의 단어들을 동일한 비율로 참고하는 것이 아니라, Decoder가 해당 시점(time step)에서 예측해야할 단어와 연관이 있는 입력 부분을 좀 더 집중(attention)해서 참고 할 수 있도록 하자는 것이 기본 아이디어이다.\n",
    "- 다양한 Attention 종류들이 있다.\n",
    "    -  Decoder에서 출력 단어를 예측하는 매 시점(time step)마다 Encoder의 입력 문장의 어느 부분에 더 집중(attention) 할지를 계산하는 방식에 따라 다양한 attention 기법이 있다.\n",
    "    -  `dot attention - Luong`, `scaled dot attention - Vaswani`, `general  attention - Luong`, `concat  attention - Bahdanau` 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca7ae-e57f-4d14-a3de-19c26436f371",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fe5670-af83-4f9d-9198-c9ebc044ea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('data/chatbot_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aed9fa62-f627-450e-b213-dc0fb2e2527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 제거\n",
    "df.drop(columns='label', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1a1c-b942-4cad-948e-80c1bd768690",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d23c37-f609-411b-aba2-17b081259cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 11823, 11823)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 질문들 + 답변들 합쳐서 학습.\n",
    "question_texts = df['Q']\n",
    "answer_texts = df['A']\n",
    "all_texts = list(question_texts + \" \"+answer_texts) # 같은 index끼리 합치기 => list로 변환\n",
    "len(question_texts), len(answer_texts), len(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ee480-100c-431c-8abd-1a63373b9950",
   "metadata": {},
   "source": [
    "## Tokenizer 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7704dca6-ec9e-48a7-8ad2-495c08aa9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 10_000\n",
    "min_frequency = 5 \n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    continuing_subword_prefix='##',            # 연결 subword 앞에 붙일 접두어지정. \n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"] \n",
    "    # [SOS]: 문장의 시작을 의미하는 토큰. [EOS]: 문장이 끝난 것을 표시.\n",
    ")\n",
    "# tokenizer: token + ##izer\n",
    "## 학습\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer) # 리스트로 부터 학습\n",
    "## tokenizer.train(\"파일경로\") # 파일에 있는 text를 학습."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0097d1d-7d62-4c81-9c09-db61741ffefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 어휘수: 7041\n"
     ]
    }
   ],
   "source": [
    "print(\"총 어휘수:\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8d72-a052-46a3-8396-e4b4c480de31",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fac5977-edbe-4c64-b0dc-0c1ee0bb4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"saved_model/vocab\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "vocab_path = os.path.join(dir_path, \"chatbot_attn_bpe.json\")\n",
    "tokenizer.save(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cf727-4a41-4e09-8bf2-74f599d86ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc1557b-9a3a-4c34-b19b-7d4106fe2132",
   "metadata": {},
   "source": [
    "# Dataset 생성\n",
    "- 한문장 단위로 학습시킬 것이므로 DataLoader를 생성하지 않고 Dataset에서 index로 조회한 질문-답변을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fd3ef9-4ad8-434d-9792-10d1ff75b53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ada0-16f7-43cd-8d2f-17bd6b03b260",
   "metadata": {},
   "source": [
    "### Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a02b67e-6be9-42ea-ba21-c8d669e8101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Attribute\n",
    "        max_length\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int - Tokenizer에 등록된 총 어휘수\n",
    "        SOS: int - [SOS] 문장의 시작 토큰 id\n",
    "        EOS: int = [EOS] 문장의 끝 토큰 id\n",
    "        question_squences: list - 모든 질문 str을 token_id_list(token sequence) 로 변환하여 저장한 list \n",
    "        answser_sequences: list - 모든 답변 str을 token_id_list(token sequence) 로 변환하여 저장한 list.\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, tokenizer, min_length=3, max_length=25):\n",
    "        \"\"\"\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\", \"답2\", ...]\n",
    "        tokenizer: Tokenizer\n",
    "        min_length=3: int - 최소 토큰 개수. 질문과 답변의 token수가 min_length 이상인 것만 학습한다.\n",
    "        max_length=25:int 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        \"\"\"\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.SOS = self.tokenizer.token_to_id('[SOS]')\n",
    "        self.EOS = self.tokenizer.token_to_id('[EOS]')\n",
    "        # 질문/답변을 토큰으로 변환한 list\n",
    "        self.question_sequences = []\n",
    "        self.answer_sequences = []\n",
    "        for q, a in zip(question_texts, answer_texts):\n",
    "            q_token = self.__process_sequence(q)\n",
    "            a_token = self.__process_sequence(a)\n",
    "            if len(q_token) > min_length and len(a_token) > min_length:\n",
    "                self.question_sequences.append(q_token)\n",
    "                self.answer_sequences.append(a_token)\n",
    "\n",
    "    def __add_special_tokens(self, token_sequence):\n",
    "        \"\"\"\n",
    "        max_length 보다 token 수가 많은 경우 max_length에 맞춰 뒤를 잘라낸다.\n",
    "        token_id_list 에 [EOS] 토큰 추가. \n",
    "        \"\"\"\n",
    "        # max_length 보다 큰 경우 줄이기. (적은 경우에는 그냥 유지.)\n",
    "        token_id_list = token_sequence[:self.max_length-1] # EOS 토큰 포함해서 max_length가 되게 하기 위해서 -1\n",
    "        token_id_list.append(self.EOS) # 마지막 토큰으로 [EOS] 추가.\n",
    "\n",
    "        return token_id_list\n",
    "\n",
    "    \n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        한 문장 string을 받아서 token_id 리스트(list)로 변환 후 반환\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        token_ids = self.__add_special_tokens(encode.ids)\n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_sequences)\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        q = torch.tensor(self.question_sequences[index], dtype=torch.int64).unsqueeze(1) # [seq_length, 1]\n",
    "        a = torch.tensor(self.answer_sequences[index], dtype=torch.int64).unsqueeze(1)\n",
    "        return q, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2221b-bd39-4fc1-8d2a-2b3bd9ef5718",
   "metadata": {},
   "source": [
    "### Dataset 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c277360c-705b-42b1-8ea2-e9f9aaca4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11714\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "MIN_LENGTH = 2\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, tokenizer, MIN_LENGTH, MAX_LENGTH)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0228fa3e-057a-45da-a583-4a45ee4f1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab7f9de1-2bf2-4ecb-99e7-7d725768086c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6119],\n",
       "        [ 379],\n",
       "        [  48],\n",
       "        [2253],\n",
       "        [   9],\n",
       "        [   3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c9392-001c-49a9-9b5b-7252531aa713",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af63e-d915-4832-840e-44a07a53cdad",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- seq2seq 모델과 동일 한 구조\n",
    "    - 이전 코드(seq2seq)와 비교해서 forward()에서 입력 처리는 token 하나씩 하나씩 처리한다. \n",
    "\n",
    "![encoder](figures/attn_encoder-network_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07a20f4-aab2-4ab0-a271-f23063d7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            num_vocabs: int - 총 어휘수 \n",
    "            hidden_size: int - GRU의 hidden size\n",
    "            embedding_dim: int - Embedding vector의 차원수 \n",
    "            num_layers: int - GRU의 layer수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers) # 단방향.(한단어씩 처리하므로)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        질문의 token한개의 토큰 id를 입력받아 hidden state를 출력\n",
    "        Parameter\n",
    "            x: 한개 토큰. shape-[1]\n",
    "            hidden: hidden state (이전 처리결과). shape: [1, 1, hidden_size]\n",
    "        Return\n",
    "            tuple: (output, hidden) - output: [1, 1, hidden_size],  hidden: [1, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        # x: [batch, 1]\n",
    "        x = self.embedding(x).unsqueeze(0) # (1, embedding_dim) -> (1, 1, embedding_dim)\\\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        # out: (seq_len, batch, hidden_size), hidden: (1, batch, hidden_size)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, device):\n",
    "        \"\"\"\n",
    "        처음 timestep에서 입력할 hidden_state. \n",
    "        값: 0\n",
    "        shape: (Bidirectional(1) x number of layers(1), batch_size: 1, hidden_size) \n",
    "        \"\"\"\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52754234-031c-48b1-adbe-120cd607b9fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53312985",
   "metadata": {},
   "source": [
    "## Attention 적용 Decoder\n",
    "![seq2seq attention outline](figures/attn_seq2seq_attention_outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8d821-0c0d-4089-b0a8-88f0d37cf014",
   "metadata": {},
   "source": [
    "- Attention은 Decoder 네트워크가 순차적으로 다음 단어를 생성하는 자기 출력의 모든 단계에서 인코더 출력 중 연관있는 부분에 **집중(attention)** 할 수 있게 한다. \n",
    "- 다양한 어텐션 기법중에 **Luong attention** 방법은 다음과 같다.\n",
    "  \n",
    "![attention decoder](figures/attn_decoder-network_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91531901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c8b7ddf-6e4d-4358-82f3-375d89544ce0",
   "metadata": {},
   "source": [
    "### Attetion Weight\n",
    "- Decoder가 현재 timestep의 단어(token)을 생성할 때 Encoder의 output 들 중 어떤 단어에 좀더 집중해야 하는지 계산하기 위한 가중치값.\n",
    "  \n",
    "![Attention Weight](figures/attn_attention_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349ed00-d090-49c3-bcf5-9792e28efdf8",
   "metadata": {},
   "source": [
    "### Attention Value\n",
    "- Decoder에서 현재 timestep의 단어를 추출할 때 사용할 Context Vector. \n",
    "    - Encoder의 output 들에 Attention Weight를 곱한다.\n",
    "    - Attention Value는 Decoder에서 단어를 생성할 때 encoder output의 어떤 단어에 더 집중하고 덜 집중할지를 가지는 값이다.\n",
    "\n",
    "![attention value](figures/attn_attention_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166d33-991d-406a-85d1-ce6575f78146",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "- Decoder의 embedding vector와 Attention Value 를 합쳐 RNN(GRU)의 입력을 만든다.\n",
    "    - **단어를 생성하기 위해 이전 timestep에서 추론한 단어(현재 timestep의 input)** 와 **Encoder output에 attention이 적용된 값** 이 둘을 합쳐 입력한다.\n",
    "    - 이 값을 Linear Layer함수+ReLU를 이용해 RNN input_size에 맞춰 준다. (어떻게 input_size에 맞출지도 학습시키기 위해 Linear Layer이용)\n",
    "\n",
    "![rnn](figures/att_attention_combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e7805-2809-48d8-a9b7-547c3f571c68",
   "metadata": {},
   "source": [
    "### 단어 예측(생성)\n",
    "- RNN에서 찾은 Feature를 총 단어개수의 units을 출력하는 Linear에 입력해 **다음 단어를 추론한다.**\n",
    "- 추론한 단어는 다음 timestep의 입력($X_t$)으로 RNN의 hidden은 다음 timestep 의 hidden state ($h_{t-1}$) 로 입력된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f57bb1-13c8-43ba-a4b8-58362fc9ca49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9653c5e5-bf2f-47ac-aa2e-63363a131e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, dropout_p, max_length):\n",
    "        super().__init__()\n",
    "        self.num_vocabs=num_vocabs # 총 단어수 (vocab size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "\n",
    "        # attention weight를 계산하는 Linear: out_features: max_length - encoder outputs의 최대개수.\n",
    "        self.attn = nn.Linear(hidden_size+embedding_dim,  max_length)    \n",
    "        # attention value 와 현재 timestep의 embedding vector를 합친것을 받아서 \n",
    "        # gru의 input을 만드는 Linear.\n",
    "        self.attn_combine = nn.Linear(hidden_size+embedding_dim , hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # gru\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # 분류기(다음 단어 예측기)\n",
    "        self.classifier = nn.Linear(hidden_size, num_vocabs)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "            x: 현재 timestep의 입력 토큰(단어) id\n",
    "            hidden: 이전 timestep 처리결과 hidden state\n",
    "            encoder_outputs: Encoder output들. \n",
    "        Return\n",
    "            tupe: (output, hidden, attention_weight)\n",
    "                output: 단어별 다음 단어일 확률.  shape: [vocab_size]\n",
    "                hidden: hidden_state. shape: [1, 1, hidden_size]\n",
    "                atttention_weight: Encoder output 중 어느 단어에 집중해야하는 지 가중치값. shape: [1, max_length]\n",
    "        \n",
    "        현재 timestep 입력과 이전 timestep 처리결과를 기준으로 encoder_output와 계산해서  encoder_output에서 집중(attention)해야할 attention value를 계산한다.\n",
    "        attention value와 현재 timestep 입력을 기준으로 단어를 추론(생성) 한다.\n",
    "        \"\"\"\n",
    "        ## embedding 작업\n",
    "        # shape:   입력 - [1](단어 한개), 출력: [1, embeding_dim] -> [1, 1, embedding_dim]\n",
    "        embedding = self.embedding(x).unsqueeze(0)\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        # attention weight 계산 입력값. embedding 와 hidden_state 합치기.\n",
    "        ## pytorch에서 tensor 합치는 함수. torch.concat((합칠대상들, ))\n",
    "        attn_in = torch.concat((embedding[0], hidden[0]), dim=1)  \n",
    "        # shape: (1, embedding_dim + hidden_size) -> attn Linear의 입력.\n",
    "        attn_score = self.attn(attn_in) # shape (1, max_length)\n",
    "        attn_weight = nn.Softmax(dim=-1)(attn_score)\n",
    "\n",
    "        # attention value를 계산. (어떤 encoder의 토큰에 더 집중할지 집중도값.)\n",
    "        ## attn_weigth x encoder의 output\n",
    "        attn_value = torch.bmm(            \n",
    "            attn_weight.unsqueeze(0), # 배치축 추가. (1, max_length)->(1,  1,          max_length)\n",
    "            encoder_outputs.unsqueeze(0) # (max_length, hidden_size)->(1, max_legnth, hidden_size)\n",
    "        )\n",
    "        # attn_value: (1, 1, hidden_size)\n",
    "    \n",
    "        ### attn_combine: attn_value + embedding_vector (합친것-concatenate)\n",
    "        ##### GRU의 현재 timestep의 입력값.\n",
    "        attn_combine_in = torch.concat([\n",
    "            attn_value[0], embedding[0]\n",
    "            # [1, hidden_size]+[1, embedding_dim] = [1, h_s + e_dim]\n",
    "        ], dim=1)  # 합치는 축 방향 지정. h+e을 합치므로 1축 지정.\n",
    "        gru_in = self.attn_combine(attn_combine_in)# 출력: [1, hidden_size]\n",
    "        gru_in = gru_in.unsqueeze(0) # [1, 1, hidden_size] : seq_len 축 추가.\n",
    "        gru_in = nn.ReLU()(gru_in)\n",
    "        #### GRU에 현재 timestep값 + attention value 넣어서 hidden_state출력(feature vec.)\n",
    "        out, hidden_state = self.gru(gru_in, hidden)\n",
    "\n",
    "        #### classifier에 out을 입력해서 다음 단어를 예측\n",
    "        last_out = self.classifier(out[0]) \n",
    "        #out: [1, 1, hidden]->[1:batch, hidden] -> last_out: [1, num_vocabs]\n",
    "\n",
    "        return last_out[0], hidden_state, attn_weight\n",
    "    \n",
    "    def initHidden(self, device):\n",
    "        # 첫번째 timestep에 넣어줄 hidden state를 생성. (0)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a69db5e2-0c5e-4ac1-a46e-2e5437e11530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, 4, 5)\n",
    "b = torch.randn(3, 5, 2)\n",
    "c = torch.bmm(a, b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0daebbf2-0daf-47bb-b286-b3eab073dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_decoder = AttentionDecoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(), \n",
    "    hidden_size=256,\n",
    "    embedding_dim=200,\n",
    "    dropout_p=0.3,\n",
    "    max_length=20\n",
    ")\n",
    "dummy_encoder = Encoder(\n",
    "    num_vocabs=tokenizer.get_vocab_size(),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=200,\n",
    "    num_layers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f517c32-f6c4-407a-abaf-7e088f0bb9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "x, y = dataset[0]\n",
    "e_hidden = dummy_encoder.init_hidden(device)\n",
    "# e_hidden.shape\n",
    "e_o, e_h = dummy_encoder(x[0], e_hidden)  # 한개 문장의 첫번째(한개) 토큰\n",
    "print(e_o.shape, e_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2223d1f2-2b55-4e24-9d51-e9b9222d88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_encoder_outputs = torch.randn(20, 256)  # 20: seq length, 256: hidden size\n",
    "d_hidden = dummy_decoder.initHidden(device)\n",
    "# d_hidden.shape\n",
    "word, d_h, attn_weight = dummy_decoder(y[0], d_hidden, dummy_encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f89e403d-8191-43db-8d7f-c84d2c2ab352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7041])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "print(word.shape)\n",
    "print(d_h.shape)\n",
    "print(attn_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42813e47-19c6-49c9-90e6-b5e3b873a5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0380, 0.0158, 0.0688, 0.0251, 0.0828, 0.0530, 0.0282, 0.0636, 0.0682,\n",
       "         0.0695, 0.0218, 0.0324, 0.0434, 0.0757, 0.0279, 0.0257, 0.0459, 0.0984,\n",
       "         0.0424, 0.0732]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7c43d-3691-4723-8051-7bc0a8a4a37e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75080eb6-da0e-4c86-998e-2dd8405e5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = dataset.tokenizer.token_to_id(\"[SOS]\")\n",
    "EOS_TOKEN = dataset.tokenizer.token_to_id(\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53184448-56e5-4c3c-8d6c-3f8f93d45076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한개 question-answer set 학습\n",
    "def train(input_tensor:\"질문한개\", target_tensor:\"답변한개\", encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, \n",
    "          loss_fn, device, max_length:\"한문장의 토큰수\", teacher_forcing_ratio=0.9):\n",
    "\n",
    "    loss = 0.0 # loss값을 저장할 변수.\n",
    "    \n",
    "    # 옵티마이저 초기화 (파라미터 초기화)\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    ###### Encoder 처리\n",
    "    encoder_hidden = encoder.initHidden(device) # 첫 timestep에 넣을 hidden state값\n",
    "    # 질문/답변의 length(토큰수) 조회\n",
    "    input_length = input_tensor.shape[0]\n",
    "    output_length = target_tensor.shape[0]\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    ########## 질문 문장의 hidden state를 토큰별로 계산해서 encoder_outputs에 저장.\n",
    "    for e_index in range(input_length):\n",
    "        encoder_out, encoder_hidden = encoder(input_tensor[e_index], encoder_hidden)\n",
    "        encoder_outputs[e_index] = encoder_out\n",
    "\n",
    "    ########### Decoder 처리\n",
    "    # 첫번째 timestep 입력 토큰: [SOS] \n",
    "    decoder_input = torch.tensor([SOS_TOKEN], device=device)\n",
    "    decoder_hidden = encoder_hidden  # 첫번째 hidden_state: encoder의 마지막 hidden state\n",
    "    teacher_forcing = True if teacher_forcing_ratio > random.random() else False\n",
    "    ##### 한개 단어씩 생성.\n",
    "    for d_index in range(output_length):\n",
    "        decoder_out, decoder_hidden, attn_weight = decoder(decoder_input, \n",
    "                                                           decoder_hidden, \n",
    "                                                           encoder_outputs)\n",
    "        # loss계산\n",
    "        loss = loss + loss_fn(decoder_out.unsqueeze(0), # [총단어수] -> [1, 총단어수]\n",
    "                              target_tensor[d_index])\n",
    "        #### 다음 timestep에 넣을 input 토큰 생성.\n",
    "        ##### teacher_forcing: True-정답 단어, False: 모델 추론 단어. ==> decoder_input에 대입\n",
    "        if teacher_forcing:\n",
    "            decoder_input = target_tensor[d_index]\n",
    "        else:\n",
    "            output_token = decoder_out.argmax(dim=-1).unsqueeze(0)\n",
    "            decoder_input = output_token.detach() # 역전파할 때 grad 계산 대상에서 빼기.\n",
    "        \n",
    "        ## teacher_forcing_rate를 업데이트 (학습이 진행될 수록 줄여준다.)\n",
    "        teacher_forcing_ratio = teacher_forcing_ratio * 0.99\n",
    "        if decoder_input == EOS_TOKEN: # 찾은 토큰이 문장의 끝. 멈추기\n",
    "            break\n",
    "    ### 순전파 완료 -> 역전파 grad계산 및 파라미터 업데이트\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # loss 평균 리턴\n",
    "    return loss.item() / output_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824a9fb-1592-44f4-8a74-6c5098bd5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterations(encoder, decoder, n_iters, dataset, device, log_interval=1000, learning_rate=0.001):\n",
    "    \n",
    "    pass\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cd1f5-0f7f-4271-9143-87b978d8e376",
   "metadata": {},
   "source": [
    "## Model 생성, 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088d4b1-67bc-42df-991e-4f8e57cb21e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639a06d-8c8c-4bab-8069-bfaeeb17f546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1f744-1929-4862-9c29-f51e28f11c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f416fa1-0a38-4a60-9c02-41261fab6cb0",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd3a41-6883-4b59-8d43-312babed0bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b844b-784a-4f15-9e39-947804573982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae49d34-7308-493d-95a4-eef87c361d33",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b25ff9-0854-4583-b4f2-0dbc84e63e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2de272-7758-4659-96bb-d5f0e6f400ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f002af5-94ab-46a3-a49e-137ac054a33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28728c0e-2013-4996-b914-f93a39fe832d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999e135-aaa9-4e1d-b517-51b3cd56799a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
