{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "-   학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다.\n",
    "-   파이토치는 **모델의 파라미터만 저장**하는 방법과 **모델 구조와 파라미터 모두를 저장**하는 두가지 방식을 제공한다.\n",
    "-   저장 함수\n",
    "    -   `torch.save(저장할 객체, 저장경로)`\n",
    "-   보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "-   저장하기\n",
    "    -   `torch.save(model, 저장경로)`\n",
    "-   불러오기\n",
    "    -   `load_model = torch.load(저장경로)`\n",
    "-   저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델의 파라미터만 저장\n",
    "\n",
    "-   모델을 구성하는 파라미터만 저장한다.\n",
    "-   모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "-   모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "\n",
    "-   모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "-   `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "-   모델의 state_dict을 조회 후 저장한다.\n",
    "    -   `torch.save(model.state_dict(), \"저장경로\")`\n",
    "-   생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    -   `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint를 저장 및 불러오기\n",
    "\n",
    "-   학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "-   Dictionary에 저장하려는 값들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "# loading된 checkpoint 값 이용해 이전 학습상태 복원\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Linear의 파라미터(weight, bias) 개수: 입력feature * 출력feature + 출력feature\n",
    "# (784, 128) 784 * 128 + 128\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(3, 4) # 3 X 4 + 4 \n",
    "        self.lr2 = nn.Linear(4, 2)\n",
    "        self.relu = nn.ReLU() # activation함수->파라미터가 없는 단순 계산함수. relu(X) = max(X, 0)\n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 저장\n",
    "torch.save(model, \"saved_models/my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_7712\\197926485.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_model = torch.load(\"saved_models/my_model.pt\")\n"
     ]
    }
   ],
   "source": [
    "## 저장된 모델 Load\n",
    "load_model = torch.load(\"saved_models/my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=4, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델에 Layer들을 조회. 모델.instance변수명\n",
    "lr_layer = model.lr1\n",
    "lr_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer에서 weight/bias 조회.  layer.weight, layer.bias\n",
    "lr1_weight = lr_layer.weight\n",
    "lr1_bias = lr_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0288, -0.5614, -0.4909],\n",
       "        [-0.3311, -0.3771,  0.4307],\n",
       "        [ 0.3548, -0.5195, -0.1379],\n",
       "        [-0.5446, -0.1220,  0.5561]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4172,  0.0534, -0.5352,  0.1600], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0288, -0.5614, -0.4909],\n",
       "        [-0.3311, -0.3771,  0.4307],\n",
       "        [ 0.3548, -0.5195, -0.1379],\n",
       "        [-0.5446, -0.1220,  0.5561]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.lr1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4172,  0.0534, -0.5352,  0.1600], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.lr1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[ 0.0288, -0.5614, -0.4909],\n",
       "                      [-0.3311, -0.3771,  0.4307],\n",
       "                      [ 0.3548, -0.5195, -0.1379],\n",
       "                      [-0.5446, -0.1220,  0.5561]])),\n",
       "             ('lr1.bias', tensor([-0.4172,  0.0534, -0.5352,  0.1600])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.1050,  0.2449,  0.0083, -0.4915],\n",
       "                      [-0.0403,  0.3696,  0.2914,  0.3962]])),\n",
       "             ('lr2.bias', tensor([-0.1641,  0.3765]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### 파라미터들(weight들, bias들)만 저장/불러기기\n",
    "# state_dict\n",
    "state_dict = model.state_dict()  # 타입: collections.OrderedDict   - [(key, value), (key, value), ..]\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr1.weight', 'lr1.bias', 'lr2.weight', 'lr2.bias'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "torch.save(state_dict, \"saved_models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_7712\\3810450207.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(\"saved_models/my_model_parameter.pt\")\n"
     ]
    }
   ],
   "source": [
    "sd = torch.load(\"saved_models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[ 0.5148,  0.4099, -0.2973],\n",
       "                      [-0.1596, -0.4820, -0.2951],\n",
       "                      [-0.2226,  0.4307,  0.0505],\n",
       "                      [-0.4605,  0.2338,  0.2596]])),\n",
       "             ('lr1.bias', tensor([ 0.5370, -0.4554, -0.0964, -0.1462])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.3830, -0.4005,  0.3755,  0.1092],\n",
       "                      [ 0.4894, -0.1427, -0.2676, -0.3616]])),\n",
       "             ('lr2.bias', tensor([-0.0450, -0.3085]))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 객체를 생성 -> load한 state_dict를 모델 파라미터에 덮어쓴다.\n",
    "new_model = MyModel()\n",
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[ 0.0288, -0.5614, -0.4909],\n",
       "                      [-0.3311, -0.3771,  0.4307],\n",
       "                      [ 0.3548, -0.5195, -0.1379],\n",
       "                      [-0.5446, -0.1220,  0.5561]])),\n",
       "             ('lr1.bias', tensor([-0.4172,  0.0534, -0.5352,  0.1600])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.1050,  0.2449,  0.0083, -0.4915],\n",
       "                      [-0.0403,  0.3696,  0.2914,  0.3962]])),\n",
       "             ('lr2.bias', tensor([-0.1641,  0.3765]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "# 파이토치 모델 구조를 조사해주는 패키지.\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyModel                                  --\n",
       "├─Linear: 1-1                            16\n",
       "├─Linear: 1-2                            10\n",
       "├─ReLU: 1-3                              --\n",
       "=================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [100, 2]                  --\n",
       "├─Linear: 1-1                            [100, 4]                  16\n",
       "├─ReLU: 1-2                              [100, 4]                  --\n",
       "├─Linear: 1-3                            [100, 2]                  10\n",
       "==========================================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (100, 3))  #(model, (input data의 shape))  (100:batch_size, 3: feature수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- 해결하려는 문제 유형에 따라 출력 Layer의 구조가 바뀐다.\n",
    "- 딥러닝 구조에서 **Feature를 추출하는 Layer 들을 Backbone** 이라고 하고 **추론하는 Layer들을 Head** 라고 한다. \n",
    "\n",
    "\n",
    "> - MLP(Multi Layer Perceptron), DNN(Deep Neural Network), ANN(Artificial Neural Network)\n",
    ">     -   Fully Connected Layer(nn.Linear)로 구성된 딥러닝 모델\n",
    ">     -   input feature들 모두에 대응하는 weight들(가중치)을 사용한다.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Regression(회귀)\n",
    "\n",
    "### Boston Housing Dataset\n",
    "\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "\n",
    "-   CRIM: 범죄율\n",
    "-   ZN: 25,000 평방피트당 주거지역 비율\n",
    "-   INDUS: 비소매 상업지구 비율\n",
    "-   CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "-   NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "-   RM: 주택당 방의 수\n",
    "-   AGE: 1940년 이전에 건설된 주택의 비율\n",
    "-   DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "-   RAD: 고속도로 접근성\n",
    "-   TAX: 재산세율\n",
    "-   PTRATIO: 학생/교사 비율\n",
    "-   B: 흑인 비율\n",
    "-   LSTAT: 하위 계층 비율\n",
    "    <br><br>\n",
    "-   **Target**\n",
    "    -   MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Dataset 준비\n",
    "## 데이터 불러오기 -> 전처리 -> Dataset -> DataLoader\n",
    "df = pd.read_csv(\"data/boston_hosing.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## X, y\n",
    "X_boston = df.drop(columns='MEDV').values\n",
    "y_boston = df['MEDV'].values\n",
    "y_boston = y_boston.reshape(-1, 1)\n",
    "X_boston.shape, y_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 -> 선형회귀기반: 전처리 - 연속형: Feature scaling, 범주형: One Hot Encoding\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 102)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dataset\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")\n",
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=404, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=102)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### 모델 정의\n",
    "# nn.Module 상속. \n",
    "# __init__(): layer 객체들 초기화, forward(): 추론 계산과정을 정의\n",
    "class BostonHousingModeling(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(13, 16)\n",
    "        self.lr2 = nn.Linear(16, 8)\n",
    "        self.lr3 = nn.Linear(8, 1)  # 최종 출력결과(집값 중위수 1개).\n",
    "        self.relu = nn.ReLU() # activation 함수.\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "        output = self.lr3(X) \n",
    "        return output\n",
    "        # 출력 layer(output)은 activation 함수를 통과시키지 않음\n",
    "        # 값의 범위가 정해져 있고 그 범위의 값을 반환하는 함수가 있을 경우는 사용할 수 있다.\n",
    "        ##  출력값 범위: 0 ~ 1 실수 - logistic(sigmoid), -1 ~ 1 사이 실수: tanh 함수\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonHousingModeling                    [404, 1]                  --\n",
       "├─Linear: 1-1                            [404, 16]                 224\n",
       "├─ReLU: 1-2                              [404, 16]                 --\n",
       "├─Linear: 1-3                            [404, 8]                  136\n",
       "├─ReLU: 1-4                              [404, 8]                  --\n",
       "├─Linear: 1-5                            [404, 1]                  9\n",
       "==========================================================================================\n",
       "Total params: 369\n",
       "Trainable params: 369\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.10\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 객체 생성\n",
    "boston_model = BostonHousingModeling().to(device)\n",
    "summary(boston_model, (404, 13), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습: loss_fn(회귀: nn.MSELoss, nn.functional.mse_loss), optimizer\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(boston_model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss() # 회귀: Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/1000] - train loss: 605.1829223632812, valid loss: 583.3720092773438\n",
      "[0002/1000] - train loss: 604.84228515625, valid loss: 583.0517578125\n",
      "[0003/1000] - train loss: 604.4996948242188, valid loss: 582.7281494140625\n",
      "[0004/1000] - train loss: 604.1549072265625, valid loss: 582.3994140625\n",
      "[0005/1000] - train loss: 603.8070678710938, valid loss: 582.0691528320312\n",
      "[0006/1000] - train loss: 603.4557495117188, valid loss: 581.7369384765625\n",
      "[0007/1000] - train loss: 603.0997924804688, valid loss: 581.4033203125\n",
      "[0008/1000] - train loss: 602.7415161132812, valid loss: 581.0679321289062\n",
      "[0009/1000] - train loss: 602.3812255859375, valid loss: 580.73095703125\n",
      "[0010/1000] - train loss: 602.0177612304688, valid loss: 580.392333984375\n",
      "[0011/1000] - train loss: 601.6516723632812, valid loss: 580.0525512695312\n",
      "[0012/1000] - train loss: 601.2818603515625, valid loss: 579.7083129882812\n",
      "[0013/1000] - train loss: 600.9086303710938, valid loss: 579.3601684570312\n",
      "[0014/1000] - train loss: 600.5318603515625, valid loss: 579.0086059570312\n",
      "[0015/1000] - train loss: 600.150390625, valid loss: 578.6539916992188\n",
      "[0016/1000] - train loss: 599.764404296875, valid loss: 578.2991943359375\n",
      "[0017/1000] - train loss: 599.3742065429688, valid loss: 577.9410400390625\n",
      "[0018/1000] - train loss: 598.9793701171875, valid loss: 577.5804443359375\n",
      "[0019/1000] - train loss: 598.579833984375, valid loss: 577.2156372070312\n",
      "[0020/1000] - train loss: 598.1747436523438, valid loss: 576.8456420898438\n",
      "[0021/1000] - train loss: 597.761962890625, valid loss: 576.4700317382812\n",
      "[0022/1000] - train loss: 597.3424682617188, valid loss: 576.0879516601562\n",
      "[0023/1000] - train loss: 596.916015625, valid loss: 575.6990356445312\n",
      "[0024/1000] - train loss: 596.4822998046875, valid loss: 575.303466796875\n",
      "[0025/1000] - train loss: 596.0408935546875, valid loss: 574.90087890625\n",
      "[0026/1000] - train loss: 595.5897827148438, valid loss: 574.4891357421875\n",
      "[0027/1000] - train loss: 595.1297607421875, valid loss: 574.0698852539062\n",
      "[0028/1000] - train loss: 594.6616821289062, valid loss: 573.6438598632812\n",
      "[0029/1000] - train loss: 594.184814453125, valid loss: 573.21044921875\n",
      "[0030/1000] - train loss: 593.6990966796875, valid loss: 572.7683715820312\n",
      "[0031/1000] - train loss: 593.2040405273438, valid loss: 572.3181762695312\n",
      "[0032/1000] - train loss: 592.6989135742188, valid loss: 571.859130859375\n",
      "[0033/1000] - train loss: 592.1849365234375, valid loss: 571.3894653320312\n",
      "[0034/1000] - train loss: 591.6630249023438, valid loss: 570.91015625\n",
      "[0035/1000] - train loss: 591.1320190429688, valid loss: 570.4192504882812\n",
      "[0036/1000] - train loss: 590.5899658203125, valid loss: 569.9154663085938\n",
      "[0037/1000] - train loss: 590.0366821289062, valid loss: 569.4017333984375\n",
      "[0038/1000] - train loss: 589.472900390625, valid loss: 568.8783569335938\n",
      "[0039/1000] - train loss: 588.8982543945312, valid loss: 568.3468017578125\n",
      "[0040/1000] - train loss: 588.3088989257812, valid loss: 567.8043212890625\n",
      "[0041/1000] - train loss: 587.7052001953125, valid loss: 567.2514038085938\n",
      "[0042/1000] - train loss: 587.0883178710938, valid loss: 566.6837158203125\n",
      "[0043/1000] - train loss: 586.4566650390625, valid loss: 566.1028442382812\n",
      "[0044/1000] - train loss: 585.8115844726562, valid loss: 565.5068359375\n",
      "[0045/1000] - train loss: 585.1492919921875, valid loss: 564.89892578125\n",
      "[0046/1000] - train loss: 584.4698486328125, valid loss: 564.27685546875\n",
      "[0047/1000] - train loss: 583.775390625, valid loss: 563.6402587890625\n",
      "[0048/1000] - train loss: 583.0653686523438, valid loss: 562.9892578125\n",
      "[0049/1000] - train loss: 582.34033203125, valid loss: 562.3245849609375\n",
      "[0050/1000] - train loss: 581.599609375, valid loss: 561.6435546875\n",
      "[0051/1000] - train loss: 580.8413696289062, valid loss: 560.9454956054688\n",
      "[0052/1000] - train loss: 580.0634155273438, valid loss: 560.2343139648438\n",
      "[0053/1000] - train loss: 579.2677001953125, valid loss: 559.5084838867188\n",
      "[0054/1000] - train loss: 578.455078125, valid loss: 558.7669677734375\n",
      "[0055/1000] - train loss: 577.6242065429688, valid loss: 558.0083618164062\n",
      "[0056/1000] - train loss: 576.7749633789062, valid loss: 557.2319946289062\n",
      "[0057/1000] - train loss: 575.908203125, valid loss: 556.4390869140625\n",
      "[0058/1000] - train loss: 575.0225219726562, valid loss: 555.6318359375\n",
      "[0059/1000] - train loss: 574.119140625, valid loss: 554.8097534179688\n",
      "[0060/1000] - train loss: 573.1980590820312, valid loss: 553.9728393554688\n",
      "[0061/1000] - train loss: 572.2578735351562, valid loss: 553.1182250976562\n",
      "[0062/1000] - train loss: 571.2989501953125, valid loss: 552.2473754882812\n",
      "[0063/1000] - train loss: 570.3214721679688, valid loss: 551.3577880859375\n",
      "[0064/1000] - train loss: 569.3245239257812, valid loss: 550.4501953125\n",
      "[0065/1000] - train loss: 568.3068237304688, valid loss: 549.5241088867188\n",
      "[0066/1000] - train loss: 567.27001953125, valid loss: 548.5813598632812\n",
      "[0067/1000] - train loss: 566.2123413085938, valid loss: 547.6201782226562\n",
      "[0068/1000] - train loss: 565.134033203125, valid loss: 546.6412963867188\n",
      "[0069/1000] - train loss: 564.0347900390625, valid loss: 545.6443481445312\n",
      "[0070/1000] - train loss: 562.9149780273438, valid loss: 544.6260375976562\n",
      "[0071/1000] - train loss: 561.7737426757812, valid loss: 543.5879516601562\n",
      "[0072/1000] - train loss: 560.6104736328125, valid loss: 542.5304565429688\n",
      "[0073/1000] - train loss: 559.4251708984375, valid loss: 541.4509887695312\n",
      "[0074/1000] - train loss: 558.2179565429688, valid loss: 540.351806640625\n",
      "[0075/1000] - train loss: 556.9878540039062, valid loss: 539.2326049804688\n",
      "[0076/1000] - train loss: 555.7343139648438, valid loss: 538.0921630859375\n",
      "[0077/1000] - train loss: 554.45703125, valid loss: 536.9293823242188\n",
      "[0078/1000] - train loss: 553.1553955078125, valid loss: 535.744384765625\n",
      "[0079/1000] - train loss: 551.8289794921875, valid loss: 534.5382080078125\n",
      "[0080/1000] - train loss: 550.4782104492188, valid loss: 533.310791015625\n",
      "[0081/1000] - train loss: 549.1032104492188, valid loss: 532.0613403320312\n",
      "[0082/1000] - train loss: 547.7030639648438, valid loss: 530.7870483398438\n",
      "[0083/1000] - train loss: 546.2780151367188, valid loss: 529.4898681640625\n",
      "[0084/1000] - train loss: 544.8272705078125, valid loss: 528.1697387695312\n",
      "[0085/1000] - train loss: 543.3501586914062, valid loss: 526.8262939453125\n",
      "[0086/1000] - train loss: 541.8455810546875, valid loss: 525.4594116210938\n",
      "[0087/1000] - train loss: 540.313720703125, valid loss: 524.0669555664062\n",
      "[0088/1000] - train loss: 538.7550048828125, valid loss: 522.64990234375\n",
      "[0089/1000] - train loss: 537.1692504882812, valid loss: 521.2078857421875\n",
      "[0090/1000] - train loss: 535.5556640625, valid loss: 519.7416381835938\n",
      "[0091/1000] - train loss: 533.9140625, valid loss: 518.2506713867188\n",
      "[0092/1000] - train loss: 532.2431640625, valid loss: 516.7330322265625\n",
      "[0093/1000] - train loss: 530.5436401367188, valid loss: 515.1887817382812\n",
      "[0094/1000] - train loss: 528.8150024414062, valid loss: 513.6181640625\n",
      "[0095/1000] - train loss: 527.056640625, valid loss: 512.02197265625\n",
      "[0096/1000] - train loss: 525.268798828125, valid loss: 510.39990234375\n",
      "[0097/1000] - train loss: 523.4498901367188, valid loss: 508.7502746582031\n",
      "[0098/1000] - train loss: 521.5993041992188, valid loss: 507.071533203125\n",
      "[0099/1000] - train loss: 519.7172241210938, valid loss: 505.365478515625\n",
      "[0100/1000] - train loss: 517.80224609375, valid loss: 503.6272888183594\n",
      "[0101/1000] - train loss: 515.8521728515625, valid loss: 501.8581848144531\n",
      "[0102/1000] - train loss: 513.8671875, valid loss: 500.0575866699219\n",
      "[0103/1000] - train loss: 511.84637451171875, valid loss: 498.2235107421875\n",
      "[0104/1000] - train loss: 509.7899475097656, valid loss: 496.3589782714844\n",
      "[0105/1000] - train loss: 507.69830322265625, valid loss: 494.4623107910156\n",
      "[0106/1000] - train loss: 505.5686950683594, valid loss: 492.5333251953125\n",
      "[0107/1000] - train loss: 503.4020690917969, valid loss: 490.5704345703125\n",
      "[0108/1000] - train loss: 501.2004089355469, valid loss: 488.5743408203125\n",
      "[0109/1000] - train loss: 498.9634094238281, valid loss: 486.5472106933594\n",
      "[0110/1000] - train loss: 496.6899108886719, valid loss: 484.4862060546875\n",
      "[0111/1000] - train loss: 494.379150390625, valid loss: 482.3924865722656\n",
      "[0112/1000] - train loss: 492.0324401855469, valid loss: 480.26727294921875\n",
      "[0113/1000] - train loss: 489.6496887207031, valid loss: 478.1097106933594\n",
      "[0114/1000] - train loss: 487.23095703125, valid loss: 475.91748046875\n",
      "[0115/1000] - train loss: 484.7765808105469, valid loss: 473.692626953125\n",
      "[0116/1000] - train loss: 482.28558349609375, valid loss: 471.4352722167969\n",
      "[0117/1000] - train loss: 479.758544921875, valid loss: 469.1442565917969\n",
      "[0118/1000] - train loss: 477.1947021484375, valid loss: 466.8216857910156\n",
      "[0119/1000] - train loss: 474.5945129394531, valid loss: 464.4660949707031\n",
      "[0120/1000] - train loss: 471.95770263671875, valid loss: 462.0772399902344\n",
      "[0121/1000] - train loss: 469.2836608886719, valid loss: 459.6543884277344\n",
      "[0122/1000] - train loss: 466.5727844238281, valid loss: 457.1973876953125\n",
      "[0123/1000] - train loss: 463.82470703125, valid loss: 454.7073059082031\n",
      "[0124/1000] - train loss: 461.03863525390625, valid loss: 452.1835632324219\n",
      "[0125/1000] - train loss: 458.2145080566406, valid loss: 449.6248474121094\n",
      "[0126/1000] - train loss: 455.3528747558594, valid loss: 447.0307922363281\n",
      "[0127/1000] - train loss: 452.4538879394531, valid loss: 444.4034118652344\n",
      "[0128/1000] - train loss: 449.5180358886719, valid loss: 441.7445373535156\n",
      "[0129/1000] - train loss: 446.5450134277344, valid loss: 439.0526428222656\n",
      "[0130/1000] - train loss: 443.5348205566406, valid loss: 436.328857421875\n",
      "[0131/1000] - train loss: 440.4886169433594, valid loss: 433.5726623535156\n",
      "[0132/1000] - train loss: 437.40582275390625, valid loss: 430.7843017578125\n",
      "[0133/1000] - train loss: 434.2859191894531, valid loss: 427.96051025390625\n",
      "[0134/1000] - train loss: 431.1300354003906, valid loss: 425.10430908203125\n",
      "[0135/1000] - train loss: 427.9382019042969, valid loss: 422.21600341796875\n",
      "[0136/1000] - train loss: 424.7110290527344, valid loss: 419.2965393066406\n",
      "[0137/1000] - train loss: 421.4486083984375, valid loss: 416.34539794921875\n",
      "[0138/1000] - train loss: 418.15191650390625, valid loss: 413.36297607421875\n",
      "[0139/1000] - train loss: 414.8202209472656, valid loss: 410.3515930175781\n",
      "[0140/1000] - train loss: 411.45538330078125, valid loss: 407.3105163574219\n",
      "[0141/1000] - train loss: 408.0584716796875, valid loss: 404.240234375\n",
      "[0142/1000] - train loss: 404.6277160644531, valid loss: 401.1387023925781\n",
      "[0143/1000] - train loss: 401.1641540527344, valid loss: 398.0058288574219\n",
      "[0144/1000] - train loss: 397.6675720214844, valid loss: 394.8450012207031\n",
      "[0145/1000] - train loss: 394.13995361328125, valid loss: 391.65765380859375\n",
      "[0146/1000] - train loss: 390.5809020996094, valid loss: 388.4446716308594\n",
      "[0147/1000] - train loss: 386.9920349121094, valid loss: 385.20263671875\n",
      "[0148/1000] - train loss: 383.37371826171875, valid loss: 381.9340515136719\n",
      "[0149/1000] - train loss: 379.72747802734375, valid loss: 378.64111328125\n",
      "[0150/1000] - train loss: 376.0539855957031, valid loss: 375.3263244628906\n",
      "[0151/1000] - train loss: 372.3533020019531, valid loss: 371.9906311035156\n",
      "[0152/1000] - train loss: 368.6258544921875, valid loss: 368.6331481933594\n",
      "[0153/1000] - train loss: 364.875244140625, valid loss: 365.25213623046875\n",
      "[0154/1000] - train loss: 361.10186767578125, valid loss: 361.85076904296875\n",
      "[0155/1000] - train loss: 357.3068542480469, valid loss: 358.4309997558594\n",
      "[0156/1000] - train loss: 353.49090576171875, valid loss: 354.9920349121094\n",
      "[0157/1000] - train loss: 349.6542663574219, valid loss: 351.5333557128906\n",
      "[0158/1000] - train loss: 345.7992858886719, valid loss: 348.06036376953125\n",
      "[0159/1000] - train loss: 341.9267578125, valid loss: 344.5738220214844\n",
      "[0160/1000] - train loss: 338.0378112792969, valid loss: 341.0738830566406\n",
      "[0161/1000] - train loss: 334.1357421875, valid loss: 337.56341552734375\n",
      "[0162/1000] - train loss: 330.22064208984375, valid loss: 334.044189453125\n",
      "[0163/1000] - train loss: 326.2947082519531, valid loss: 330.51788330078125\n",
      "[0164/1000] - train loss: 322.3595886230469, valid loss: 326.9826965332031\n",
      "[0165/1000] - train loss: 318.4153137207031, valid loss: 323.4433898925781\n",
      "[0166/1000] - train loss: 314.4627685546875, valid loss: 319.899658203125\n",
      "[0167/1000] - train loss: 310.5046691894531, valid loss: 316.3475341796875\n",
      "[0168/1000] - train loss: 306.54364013671875, valid loss: 312.79620361328125\n",
      "[0169/1000] - train loss: 302.58251953125, valid loss: 309.2442626953125\n",
      "[0170/1000] - train loss: 298.6222229003906, valid loss: 305.69378662109375\n",
      "[0171/1000] - train loss: 294.6655578613281, valid loss: 302.1468200683594\n",
      "[0172/1000] - train loss: 290.7127990722656, valid loss: 298.60302734375\n",
      "[0173/1000] - train loss: 286.7669372558594, valid loss: 295.0657653808594\n",
      "[0174/1000] - train loss: 282.82928466796875, valid loss: 291.5389404296875\n",
      "[0175/1000] - train loss: 278.9005432128906, valid loss: 288.0203552246094\n",
      "[0176/1000] - train loss: 274.9844970703125, valid loss: 284.5127868652344\n",
      "[0177/1000] - train loss: 271.08367919921875, valid loss: 281.01873779296875\n",
      "[0178/1000] - train loss: 267.199951171875, valid loss: 277.54052734375\n",
      "[0179/1000] - train loss: 263.3343811035156, valid loss: 274.0809020996094\n",
      "[0180/1000] - train loss: 259.4894104003906, valid loss: 270.6427001953125\n",
      "[0181/1000] - train loss: 255.66595458984375, valid loss: 267.22705078125\n",
      "[0182/1000] - train loss: 251.8645782470703, valid loss: 263.835693359375\n",
      "[0183/1000] - train loss: 248.08895874023438, valid loss: 260.4684753417969\n",
      "[0184/1000] - train loss: 244.341552734375, valid loss: 257.1275329589844\n",
      "[0185/1000] - train loss: 240.62411499023438, valid loss: 253.81549072265625\n",
      "[0186/1000] - train loss: 236.93734741210938, valid loss: 250.5343017578125\n",
      "[0187/1000] - train loss: 233.28411865234375, valid loss: 247.28543090820312\n",
      "[0188/1000] - train loss: 229.6671142578125, valid loss: 244.0695343017578\n",
      "[0189/1000] - train loss: 226.0872344970703, valid loss: 240.88790893554688\n",
      "[0190/1000] - train loss: 222.5454559326172, valid loss: 237.73667907714844\n",
      "[0191/1000] - train loss: 219.0435028076172, valid loss: 234.61660766601562\n",
      "[0192/1000] - train loss: 215.58334350585938, valid loss: 231.5336456298828\n",
      "[0193/1000] - train loss: 212.1665496826172, valid loss: 228.49046325683594\n",
      "[0194/1000] - train loss: 208.7945556640625, valid loss: 225.48837280273438\n",
      "[0195/1000] - train loss: 205.46859741210938, valid loss: 222.5282440185547\n",
      "[0196/1000] - train loss: 202.18780517578125, valid loss: 219.60977172851562\n",
      "[0197/1000] - train loss: 198.95494079589844, valid loss: 216.728271484375\n",
      "[0198/1000] - train loss: 195.77139282226562, valid loss: 213.8905487060547\n",
      "[0199/1000] - train loss: 192.6387939453125, valid loss: 211.0980224609375\n",
      "[0200/1000] - train loss: 189.55787658691406, valid loss: 208.35162353515625\n",
      "[0201/1000] - train loss: 186.53053283691406, valid loss: 205.65234375\n",
      "[0202/1000] - train loss: 183.55755615234375, valid loss: 203.0007781982422\n",
      "[0203/1000] - train loss: 180.63949584960938, valid loss: 200.39739990234375\n",
      "[0204/1000] - train loss: 177.7769775390625, valid loss: 197.8426055908203\n",
      "[0205/1000] - train loss: 174.97021484375, valid loss: 195.33651733398438\n",
      "[0206/1000] - train loss: 172.2199249267578, valid loss: 192.8784637451172\n",
      "[0207/1000] - train loss: 169.52621459960938, valid loss: 190.46890258789062\n",
      "[0208/1000] - train loss: 166.88900756835938, valid loss: 188.10845947265625\n",
      "[0209/1000] - train loss: 164.3086395263672, valid loss: 185.7969512939453\n",
      "[0210/1000] - train loss: 161.78492736816406, valid loss: 183.5342559814453\n",
      "[0211/1000] - train loss: 159.317626953125, valid loss: 181.320068359375\n",
      "[0212/1000] - train loss: 156.90660095214844, valid loss: 179.15406799316406\n",
      "[0213/1000] - train loss: 154.55113220214844, valid loss: 177.03640747070312\n",
      "[0214/1000] - train loss: 152.2496795654297, valid loss: 174.9659881591797\n",
      "[0215/1000] - train loss: 150.00201416015625, valid loss: 172.94210815429688\n",
      "[0216/1000] - train loss: 147.80796813964844, valid loss: 170.9638671875\n",
      "[0217/1000] - train loss: 145.6656036376953, valid loss: 169.03062438964844\n",
      "[0218/1000] - train loss: 143.57603454589844, valid loss: 167.14175415039062\n",
      "[0219/1000] - train loss: 141.53810119628906, valid loss: 165.2959747314453\n",
      "[0220/1000] - train loss: 139.55088806152344, valid loss: 163.49134826660156\n",
      "[0221/1000] - train loss: 137.61322021484375, valid loss: 161.72792053222656\n",
      "[0222/1000] - train loss: 135.72421264648438, valid loss: 160.00527954101562\n",
      "[0223/1000] - train loss: 133.8828582763672, valid loss: 158.32228088378906\n",
      "[0224/1000] - train loss: 132.08798217773438, valid loss: 156.67770385742188\n",
      "[0225/1000] - train loss: 130.33856201171875, valid loss: 155.07164001464844\n",
      "[0226/1000] - train loss: 128.6332550048828, valid loss: 153.5031280517578\n",
      "[0227/1000] - train loss: 126.97098541259766, valid loss: 151.97128295898438\n",
      "[0228/1000] - train loss: 125.35104370117188, valid loss: 150.474853515625\n",
      "[0229/1000] - train loss: 123.77147674560547, valid loss: 149.0126495361328\n",
      "[0230/1000] - train loss: 122.23165130615234, valid loss: 147.5834503173828\n",
      "[0231/1000] - train loss: 120.73038482666016, valid loss: 146.18653869628906\n",
      "[0232/1000] - train loss: 119.26593780517578, valid loss: 144.8211669921875\n",
      "[0233/1000] - train loss: 117.8371810913086, valid loss: 143.48638916015625\n",
      "[0234/1000] - train loss: 116.4432373046875, valid loss: 142.1812286376953\n",
      "[0235/1000] - train loss: 115.08275604248047, valid loss: 140.9044647216797\n",
      "[0236/1000] - train loss: 113.75353240966797, valid loss: 139.6538543701172\n",
      "[0237/1000] - train loss: 112.45499420166016, valid loss: 138.4298095703125\n",
      "[0238/1000] - train loss: 111.18646240234375, valid loss: 137.2314910888672\n",
      "[0239/1000] - train loss: 109.94686889648438, valid loss: 136.05813598632812\n",
      "[0240/1000] - train loss: 108.7352523803711, valid loss: 134.9083251953125\n",
      "[0241/1000] - train loss: 107.55011749267578, valid loss: 133.78111267089844\n",
      "[0242/1000] - train loss: 106.39124298095703, valid loss: 132.67630004882812\n",
      "[0243/1000] - train loss: 105.25758361816406, valid loss: 131.5918426513672\n",
      "[0244/1000] - train loss: 104.148681640625, valid loss: 130.52825927734375\n",
      "[0245/1000] - train loss: 103.0628662109375, valid loss: 129.48464965820312\n",
      "[0246/1000] - train loss: 101.99897766113281, valid loss: 128.46043395996094\n",
      "[0247/1000] - train loss: 100.95635223388672, valid loss: 127.4539794921875\n",
      "[0248/1000] - train loss: 99.93411254882812, valid loss: 126.46504211425781\n",
      "[0249/1000] - train loss: 98.93037414550781, valid loss: 125.49268341064453\n",
      "[0250/1000] - train loss: 97.94566345214844, valid loss: 124.53680419921875\n",
      "[0251/1000] - train loss: 96.9791030883789, valid loss: 123.59716033935547\n",
      "[0252/1000] - train loss: 96.02992248535156, valid loss: 122.67327117919922\n",
      "[0253/1000] - train loss: 95.0982666015625, valid loss: 121.76407623291016\n",
      "[0254/1000] - train loss: 94.18273162841797, valid loss: 120.8696517944336\n",
      "[0255/1000] - train loss: 93.2822265625, valid loss: 119.98955535888672\n",
      "[0256/1000] - train loss: 92.39669799804688, valid loss: 119.1225814819336\n",
      "[0257/1000] - train loss: 91.5257797241211, valid loss: 118.26849365234375\n",
      "[0258/1000] - train loss: 90.66814422607422, valid loss: 117.42567443847656\n",
      "[0259/1000] - train loss: 89.82401275634766, valid loss: 116.59479522705078\n",
      "[0260/1000] - train loss: 88.99317169189453, valid loss: 115.77552795410156\n",
      "[0261/1000] - train loss: 88.17520904541016, valid loss: 114.9665756225586\n",
      "[0262/1000] - train loss: 87.36898803710938, valid loss: 114.1681137084961\n",
      "[0263/1000] - train loss: 86.57390594482422, valid loss: 113.3809585571289\n",
      "[0264/1000] - train loss: 85.79020690917969, valid loss: 112.60465240478516\n",
      "[0265/1000] - train loss: 85.01776885986328, valid loss: 111.83841705322266\n",
      "[0266/1000] - train loss: 84.2562255859375, valid loss: 111.08267211914062\n",
      "[0267/1000] - train loss: 83.50511169433594, valid loss: 110.33710479736328\n",
      "[0268/1000] - train loss: 82.76361083984375, valid loss: 109.6015625\n",
      "[0269/1000] - train loss: 82.03225708007812, valid loss: 108.87570190429688\n",
      "[0270/1000] - train loss: 81.3094253540039, valid loss: 108.15913391113281\n",
      "[0271/1000] - train loss: 80.59564971923828, valid loss: 107.45201110839844\n",
      "[0272/1000] - train loss: 79.89065551757812, valid loss: 106.7540054321289\n",
      "[0273/1000] - train loss: 79.19456481933594, valid loss: 106.0645523071289\n",
      "[0274/1000] - train loss: 78.5072250366211, valid loss: 105.38224792480469\n",
      "[0275/1000] - train loss: 77.82862854003906, valid loss: 104.70818328857422\n",
      "[0276/1000] - train loss: 77.1574478149414, valid loss: 104.04240417480469\n",
      "[0277/1000] - train loss: 76.49327087402344, valid loss: 103.38484954833984\n",
      "[0278/1000] - train loss: 75.83645629882812, valid loss: 102.73554229736328\n",
      "[0279/1000] - train loss: 75.18728637695312, valid loss: 102.09437561035156\n",
      "[0280/1000] - train loss: 74.54496765136719, valid loss: 101.46083068847656\n",
      "[0281/1000] - train loss: 73.90987396240234, valid loss: 100.83361053466797\n",
      "[0282/1000] - train loss: 73.28140258789062, valid loss: 100.2149658203125\n",
      "[0283/1000] - train loss: 72.65989685058594, valid loss: 99.60580444335938\n",
      "[0284/1000] - train loss: 72.04557800292969, valid loss: 99.00404357910156\n",
      "[0285/1000] - train loss: 71.43819427490234, valid loss: 98.40931701660156\n",
      "[0286/1000] - train loss: 70.8371810913086, valid loss: 97.82128143310547\n",
      "[0287/1000] - train loss: 70.24256134033203, valid loss: 97.24000549316406\n",
      "[0288/1000] - train loss: 69.65470123291016, valid loss: 96.66567993164062\n",
      "[0289/1000] - train loss: 69.07339477539062, valid loss: 96.09820556640625\n",
      "[0290/1000] - train loss: 68.49855041503906, valid loss: 95.53541564941406\n",
      "[0291/1000] - train loss: 67.92955017089844, valid loss: 94.9769515991211\n",
      "[0292/1000] - train loss: 67.36654663085938, valid loss: 94.42520904541016\n",
      "[0293/1000] - train loss: 66.80936431884766, valid loss: 93.88015747070312\n",
      "[0294/1000] - train loss: 66.25798034667969, valid loss: 93.3414077758789\n",
      "[0295/1000] - train loss: 65.71205139160156, valid loss: 92.80610656738281\n",
      "[0296/1000] - train loss: 65.1718978881836, valid loss: 92.27669525146484\n",
      "[0297/1000] - train loss: 64.63751983642578, valid loss: 91.7523422241211\n",
      "[0298/1000] - train loss: 64.1092758178711, valid loss: 91.23406982421875\n",
      "[0299/1000] - train loss: 63.586883544921875, valid loss: 90.72178649902344\n",
      "[0300/1000] - train loss: 63.07065963745117, valid loss: 90.21570587158203\n",
      "[0301/1000] - train loss: 62.5602912902832, valid loss: 89.71552276611328\n",
      "[0302/1000] - train loss: 62.055049896240234, valid loss: 89.22137451171875\n",
      "[0303/1000] - train loss: 61.556068420410156, valid loss: 88.73261260986328\n",
      "[0304/1000] - train loss: 61.06245803833008, valid loss: 88.24931335449219\n",
      "[0305/1000] - train loss: 60.57453155517578, valid loss: 87.7716293334961\n",
      "[0306/1000] - train loss: 60.092079162597656, valid loss: 87.29938507080078\n",
      "[0307/1000] - train loss: 59.61539077758789, valid loss: 86.83356475830078\n",
      "[0308/1000] - train loss: 59.14402770996094, valid loss: 86.3741455078125\n",
      "[0309/1000] - train loss: 58.677425384521484, valid loss: 85.9202880859375\n",
      "[0310/1000] - train loss: 58.21614456176758, valid loss: 85.47156524658203\n",
      "[0311/1000] - train loss: 57.759605407714844, valid loss: 85.02742004394531\n",
      "[0312/1000] - train loss: 57.30839157104492, valid loss: 84.58848571777344\n",
      "[0313/1000] - train loss: 56.86275100708008, valid loss: 84.15470123291016\n",
      "[0314/1000] - train loss: 56.42265701293945, valid loss: 83.72637939453125\n",
      "[0315/1000] - train loss: 55.98783493041992, valid loss: 83.30386352539062\n",
      "[0316/1000] - train loss: 55.558067321777344, valid loss: 82.88630676269531\n",
      "[0317/1000] - train loss: 55.13358688354492, valid loss: 82.47362518310547\n",
      "[0318/1000] - train loss: 54.713863372802734, valid loss: 82.0656967163086\n",
      "[0319/1000] - train loss: 54.2993278503418, valid loss: 81.66249084472656\n",
      "[0320/1000] - train loss: 53.88959503173828, valid loss: 81.26167297363281\n",
      "[0321/1000] - train loss: 53.48441696166992, valid loss: 80.86519622802734\n",
      "[0322/1000] - train loss: 53.08341979980469, valid loss: 80.47266387939453\n",
      "[0323/1000] - train loss: 52.68659210205078, valid loss: 80.0845947265625\n",
      "[0324/1000] - train loss: 52.2945556640625, valid loss: 79.7014389038086\n",
      "[0325/1000] - train loss: 51.90715026855469, valid loss: 79.32296752929688\n",
      "[0326/1000] - train loss: 51.52428436279297, valid loss: 78.94837951660156\n",
      "[0327/1000] - train loss: 51.146018981933594, valid loss: 78.57776641845703\n",
      "[0328/1000] - train loss: 50.772499084472656, valid loss: 78.21133422851562\n",
      "[0329/1000] - train loss: 50.403778076171875, valid loss: 77.84877014160156\n",
      "[0330/1000] - train loss: 50.03948211669922, valid loss: 77.48957061767578\n",
      "[0331/1000] - train loss: 49.679378509521484, valid loss: 77.13319396972656\n",
      "[0332/1000] - train loss: 49.323638916015625, valid loss: 76.77873992919922\n",
      "[0333/1000] - train loss: 48.97209548950195, valid loss: 76.42826843261719\n",
      "[0334/1000] - train loss: 48.62454605102539, valid loss: 76.08243560791016\n",
      "[0335/1000] - train loss: 48.281158447265625, valid loss: 75.74070739746094\n",
      "[0336/1000] - train loss: 47.94245147705078, valid loss: 75.4020767211914\n",
      "[0337/1000] - train loss: 47.607872009277344, valid loss: 75.06718444824219\n",
      "[0338/1000] - train loss: 47.27750778198242, valid loss: 74.73588562011719\n",
      "[0339/1000] - train loss: 46.95112991333008, valid loss: 74.40788269042969\n",
      "[0340/1000] - train loss: 46.62890625, valid loss: 74.08314514160156\n",
      "[0341/1000] - train loss: 46.31058883666992, valid loss: 73.7619857788086\n",
      "[0342/1000] - train loss: 45.996360778808594, valid loss: 73.44435119628906\n",
      "[0343/1000] - train loss: 45.6859245300293, valid loss: 73.13018798828125\n",
      "[0344/1000] - train loss: 45.3791389465332, valid loss: 72.81947326660156\n",
      "[0345/1000] - train loss: 45.076316833496094, valid loss: 72.51216125488281\n",
      "[0346/1000] - train loss: 44.777347564697266, valid loss: 72.20861053466797\n",
      "[0347/1000] - train loss: 44.48181915283203, valid loss: 71.90887451171875\n",
      "[0348/1000] - train loss: 44.18977355957031, valid loss: 71.6125259399414\n",
      "[0349/1000] - train loss: 43.901084899902344, valid loss: 71.31949615478516\n",
      "[0350/1000] - train loss: 43.615997314453125, valid loss: 71.02964782714844\n",
      "[0351/1000] - train loss: 43.33427429199219, valid loss: 70.74284362792969\n",
      "[0352/1000] - train loss: 43.05609130859375, valid loss: 70.45899963378906\n",
      "[0353/1000] - train loss: 42.78141403198242, valid loss: 70.17826080322266\n",
      "[0354/1000] - train loss: 42.5102424621582, valid loss: 69.90060424804688\n",
      "[0355/1000] - train loss: 42.242591857910156, valid loss: 69.62559509277344\n",
      "[0356/1000] - train loss: 41.9784049987793, valid loss: 69.3533935546875\n",
      "[0357/1000] - train loss: 41.71738815307617, valid loss: 69.08422088623047\n",
      "[0358/1000] - train loss: 41.459564208984375, valid loss: 68.81974029541016\n",
      "[0359/1000] - train loss: 41.205257415771484, valid loss: 68.5582275390625\n",
      "[0360/1000] - train loss: 40.95439147949219, valid loss: 68.29930877685547\n",
      "[0361/1000] - train loss: 40.70711898803711, valid loss: 68.04322052001953\n",
      "[0362/1000] - train loss: 40.463340759277344, valid loss: 67.7900161743164\n",
      "[0363/1000] - train loss: 40.22323989868164, valid loss: 67.53966522216797\n",
      "[0364/1000] - train loss: 39.9863395690918, valid loss: 67.29212188720703\n",
      "[0365/1000] - train loss: 39.75255584716797, valid loss: 67.04703521728516\n",
      "[0366/1000] - train loss: 39.5218505859375, valid loss: 66.80447387695312\n",
      "[0367/1000] - train loss: 39.29409408569336, valid loss: 66.56465911865234\n",
      "[0368/1000] - train loss: 39.06920623779297, valid loss: 66.32750701904297\n",
      "[0369/1000] - train loss: 38.84731674194336, valid loss: 66.09297943115234\n",
      "[0370/1000] - train loss: 38.62848663330078, valid loss: 65.86103820800781\n",
      "[0371/1000] - train loss: 38.41263961791992, valid loss: 65.6308364868164\n",
      "[0372/1000] - train loss: 38.19966125488281, valid loss: 65.40364837646484\n",
      "[0373/1000] - train loss: 37.9896354675293, valid loss: 65.17898559570312\n",
      "[0374/1000] - train loss: 37.78255081176758, valid loss: 64.95664978027344\n",
      "[0375/1000] - train loss: 37.57820510864258, valid loss: 64.73673248291016\n",
      "[0376/1000] - train loss: 37.3767204284668, valid loss: 64.51919555664062\n",
      "[0377/1000] - train loss: 37.17806625366211, valid loss: 64.3040542602539\n",
      "[0378/1000] - train loss: 36.98211669921875, valid loss: 64.09125518798828\n",
      "[0379/1000] - train loss: 36.788841247558594, valid loss: 63.880767822265625\n",
      "[0380/1000] - train loss: 36.597991943359375, valid loss: 63.672550201416016\n",
      "[0381/1000] - train loss: 36.40956115722656, valid loss: 63.4665412902832\n",
      "[0382/1000] - train loss: 36.223480224609375, valid loss: 63.26156997680664\n",
      "[0383/1000] - train loss: 36.03965377807617, valid loss: 63.057647705078125\n",
      "[0384/1000] - train loss: 35.85821533203125, valid loss: 62.85592269897461\n",
      "[0385/1000] - train loss: 35.6789665222168, valid loss: 62.65639877319336\n",
      "[0386/1000] - train loss: 35.502262115478516, valid loss: 62.459083557128906\n",
      "[0387/1000] - train loss: 35.32796859741211, valid loss: 62.263938903808594\n",
      "[0388/1000] - train loss: 35.15603256225586, valid loss: 62.07059097290039\n",
      "[0389/1000] - train loss: 34.98624038696289, valid loss: 61.87876892089844\n",
      "[0390/1000] - train loss: 34.8187255859375, valid loss: 61.68880081176758\n",
      "[0391/1000] - train loss: 34.65320587158203, valid loss: 61.500858306884766\n",
      "[0392/1000] - train loss: 34.48972702026367, valid loss: 61.314693450927734\n",
      "[0393/1000] - train loss: 34.327919006347656, valid loss: 61.13032150268555\n",
      "[0394/1000] - train loss: 34.16822814941406, valid loss: 60.94775390625\n",
      "[0395/1000] - train loss: 34.01070022583008, valid loss: 60.76667404174805\n",
      "[0396/1000] - train loss: 33.85529708862305, valid loss: 60.58729553222656\n",
      "[0397/1000] - train loss: 33.702274322509766, valid loss: 60.409454345703125\n",
      "[0398/1000] - train loss: 33.55133056640625, valid loss: 60.233131408691406\n",
      "[0399/1000] - train loss: 33.40238571166992, valid loss: 60.058345794677734\n",
      "[0400/1000] - train loss: 33.25532913208008, valid loss: 59.885066986083984\n",
      "[0401/1000] - train loss: 33.110286712646484, valid loss: 59.71295166015625\n",
      "[0402/1000] - train loss: 32.96718978881836, valid loss: 59.54206466674805\n",
      "[0403/1000] - train loss: 32.8259391784668, valid loss: 59.372703552246094\n",
      "[0404/1000] - train loss: 32.68650436401367, valid loss: 59.20484924316406\n",
      "[0405/1000] - train loss: 32.548946380615234, valid loss: 59.03848648071289\n",
      "[0406/1000] - train loss: 32.413238525390625, valid loss: 58.87363052368164\n",
      "[0407/1000] - train loss: 32.27936935424805, valid loss: 58.71029281616211\n",
      "[0408/1000] - train loss: 32.147315979003906, valid loss: 58.54847717285156\n",
      "[0409/1000] - train loss: 32.01691818237305, valid loss: 58.38823699951172\n",
      "[0410/1000] - train loss: 31.888198852539062, valid loss: 58.229736328125\n",
      "[0411/1000] - train loss: 31.76114273071289, valid loss: 58.0726432800293\n",
      "[0412/1000] - train loss: 31.63559913635254, valid loss: 57.916961669921875\n",
      "[0413/1000] - train loss: 31.511518478393555, valid loss: 57.7626838684082\n",
      "[0414/1000] - train loss: 31.38897705078125, valid loss: 57.609596252441406\n",
      "[0415/1000] - train loss: 31.268009185791016, valid loss: 57.45758056640625\n",
      "[0416/1000] - train loss: 31.14846420288086, valid loss: 57.30654525756836\n",
      "[0417/1000] - train loss: 31.03046417236328, valid loss: 57.15691375732422\n",
      "[0418/1000] - train loss: 30.914011001586914, valid loss: 57.00869369506836\n",
      "[0419/1000] - train loss: 30.799116134643555, valid loss: 56.86174392700195\n",
      "[0420/1000] - train loss: 30.68572425842285, valid loss: 56.71597671508789\n",
      "[0421/1000] - train loss: 30.57381820678711, valid loss: 56.57157516479492\n",
      "[0422/1000] - train loss: 30.4633846282959, valid loss: 56.428524017333984\n",
      "[0423/1000] - train loss: 30.354400634765625, valid loss: 56.28681182861328\n",
      "[0424/1000] - train loss: 30.246850967407227, valid loss: 56.14643478393555\n",
      "[0425/1000] - train loss: 30.1406307220459, valid loss: 56.007362365722656\n",
      "[0426/1000] - train loss: 30.035673141479492, valid loss: 55.86946105957031\n",
      "[0427/1000] - train loss: 29.931922912597656, valid loss: 55.732723236083984\n",
      "[0428/1000] - train loss: 29.82952308654785, valid loss: 55.597164154052734\n",
      "[0429/1000] - train loss: 29.728408813476562, valid loss: 55.46277618408203\n",
      "[0430/1000] - train loss: 29.628551483154297, valid loss: 55.3296012878418\n",
      "[0431/1000] - train loss: 29.529966354370117, valid loss: 55.19758605957031\n",
      "[0432/1000] - train loss: 29.4326171875, valid loss: 55.066707611083984\n",
      "[0433/1000] - train loss: 29.336456298828125, valid loss: 54.93695831298828\n",
      "[0434/1000] - train loss: 29.241485595703125, valid loss: 54.8083381652832\n",
      "[0435/1000] - train loss: 29.147693634033203, valid loss: 54.680809020996094\n",
      "[0436/1000] - train loss: 29.05506706237793, valid loss: 54.554420471191406\n",
      "[0437/1000] - train loss: 28.96360206604004, valid loss: 54.42911148071289\n",
      "[0438/1000] - train loss: 28.873279571533203, valid loss: 54.30491256713867\n",
      "[0439/1000] - train loss: 28.78420066833496, valid loss: 54.18190002441406\n",
      "[0440/1000] - train loss: 28.696626663208008, valid loss: 54.06013107299805\n",
      "[0441/1000] - train loss: 28.610210418701172, valid loss: 53.939334869384766\n",
      "[0442/1000] - train loss: 28.52490234375, valid loss: 53.81950759887695\n",
      "[0443/1000] - train loss: 28.440675735473633, valid loss: 53.70070266723633\n",
      "[0444/1000] - train loss: 28.357513427734375, valid loss: 53.58280563354492\n",
      "[0445/1000] - train loss: 28.27532958984375, valid loss: 53.465110778808594\n",
      "[0446/1000] - train loss: 28.194162368774414, valid loss: 53.348350524902344\n",
      "[0447/1000] - train loss: 28.114137649536133, valid loss: 53.23243713378906\n",
      "[0448/1000] - train loss: 28.035167694091797, valid loss: 53.11747360229492\n",
      "[0449/1000] - train loss: 27.957191467285156, valid loss: 53.003475189208984\n",
      "[0450/1000] - train loss: 27.88017463684082, valid loss: 52.890289306640625\n",
      "[0451/1000] - train loss: 27.804100036621094, valid loss: 52.777835845947266\n",
      "[0452/1000] - train loss: 27.72890281677246, valid loss: 52.66668701171875\n",
      "[0453/1000] - train loss: 27.654516220092773, valid loss: 52.55659866333008\n",
      "[0454/1000] - train loss: 27.58103370666504, valid loss: 52.44745635986328\n",
      "[0455/1000] - train loss: 27.508373260498047, valid loss: 52.33926010131836\n",
      "[0456/1000] - train loss: 27.436567306518555, valid loss: 52.23200225830078\n",
      "[0457/1000] - train loss: 27.365589141845703, valid loss: 52.12554168701172\n",
      "[0458/1000] - train loss: 27.29541778564453, valid loss: 52.0198860168457\n",
      "[0459/1000] - train loss: 27.226030349731445, valid loss: 51.9151496887207\n",
      "[0460/1000] - train loss: 27.157367706298828, valid loss: 51.81129837036133\n",
      "[0461/1000] - train loss: 27.089391708374023, valid loss: 51.7086296081543\n",
      "[0462/1000] - train loss: 27.022178649902344, valid loss: 51.60698318481445\n",
      "[0463/1000] - train loss: 26.955684661865234, valid loss: 51.50627899169922\n",
      "[0464/1000] - train loss: 26.889881134033203, valid loss: 51.405879974365234\n",
      "[0465/1000] - train loss: 26.824777603149414, valid loss: 51.305782318115234\n",
      "[0466/1000] - train loss: 26.760311126708984, valid loss: 51.20656204223633\n",
      "[0467/1000] - train loss: 26.696531295776367, valid loss: 51.108211517333984\n",
      "[0468/1000] - train loss: 26.633344650268555, valid loss: 51.01072311401367\n",
      "[0469/1000] - train loss: 26.570831298828125, valid loss: 50.9140510559082\n",
      "[0470/1000] - train loss: 26.508962631225586, valid loss: 50.81804656982422\n",
      "[0471/1000] - train loss: 26.44771957397461, valid loss: 50.7228889465332\n",
      "[0472/1000] - train loss: 26.387073516845703, valid loss: 50.628570556640625\n",
      "[0473/1000] - train loss: 26.327104568481445, valid loss: 50.53492736816406\n",
      "[0474/1000] - train loss: 26.267786026000977, valid loss: 50.44197463989258\n",
      "[0475/1000] - train loss: 26.209074020385742, valid loss: 50.34967041015625\n",
      "[0476/1000] - train loss: 26.151138305664062, valid loss: 50.258018493652344\n",
      "[0477/1000] - train loss: 26.09379768371582, valid loss: 50.16700744628906\n",
      "[0478/1000] - train loss: 26.037128448486328, valid loss: 50.07666015625\n",
      "[0479/1000] - train loss: 25.981121063232422, valid loss: 49.98698425292969\n",
      "[0480/1000] - train loss: 25.925737380981445, valid loss: 49.89796829223633\n",
      "[0481/1000] - train loss: 25.870933532714844, valid loss: 49.8095703125\n",
      "[0482/1000] - train loss: 25.816638946533203, valid loss: 49.72157287597656\n",
      "[0483/1000] - train loss: 25.762868881225586, valid loss: 49.634071350097656\n",
      "[0484/1000] - train loss: 25.709672927856445, valid loss: 49.547210693359375\n",
      "[0485/1000] - train loss: 25.657028198242188, valid loss: 49.46110916137695\n",
      "[0486/1000] - train loss: 25.604949951171875, valid loss: 49.37583923339844\n",
      "[0487/1000] - train loss: 25.55340003967285, valid loss: 49.29117965698242\n",
      "[0488/1000] - train loss: 25.50234031677246, valid loss: 49.207130432128906\n",
      "[0489/1000] - train loss: 25.451766967773438, valid loss: 49.123680114746094\n",
      "[0490/1000] - train loss: 25.40172004699707, valid loss: 49.04051208496094\n",
      "[0491/1000] - train loss: 25.352144241333008, valid loss: 48.95759201049805\n",
      "[0492/1000] - train loss: 25.303016662597656, valid loss: 48.875244140625\n",
      "[0493/1000] - train loss: 25.25431251525879, valid loss: 48.79365921020508\n",
      "[0494/1000] - train loss: 25.205991744995117, valid loss: 48.712615966796875\n",
      "[0495/1000] - train loss: 25.158031463623047, valid loss: 48.63214111328125\n",
      "[0496/1000] - train loss: 25.11039924621582, valid loss: 48.55238342285156\n",
      "[0497/1000] - train loss: 25.063159942626953, valid loss: 48.47309112548828\n",
      "[0498/1000] - train loss: 25.016311645507812, valid loss: 48.394287109375\n",
      "[0499/1000] - train loss: 24.969850540161133, valid loss: 48.31599426269531\n",
      "[0500/1000] - train loss: 24.923748016357422, valid loss: 48.23811721801758\n",
      "[0501/1000] - train loss: 24.877975463867188, valid loss: 48.16057586669922\n",
      "[0502/1000] - train loss: 24.832544326782227, valid loss: 48.083126068115234\n",
      "[0503/1000] - train loss: 24.78743553161621, valid loss: 48.005950927734375\n",
      "[0504/1000] - train loss: 24.742666244506836, valid loss: 47.929229736328125\n",
      "[0505/1000] - train loss: 24.69823455810547, valid loss: 47.85292053222656\n",
      "[0506/1000] - train loss: 24.65413475036621, valid loss: 47.777076721191406\n",
      "[0507/1000] - train loss: 24.61040687561035, valid loss: 47.70199966430664\n",
      "[0508/1000] - train loss: 24.567075729370117, valid loss: 47.62749099731445\n",
      "[0509/1000] - train loss: 24.524017333984375, valid loss: 47.55353927612305\n",
      "[0510/1000] - train loss: 24.481281280517578, valid loss: 47.4802131652832\n",
      "[0511/1000] - train loss: 24.438854217529297, valid loss: 47.4074821472168\n",
      "[0512/1000] - train loss: 24.39673614501953, valid loss: 47.33530044555664\n",
      "[0513/1000] - train loss: 24.354883193969727, valid loss: 47.263465881347656\n",
      "[0514/1000] - train loss: 24.313278198242188, valid loss: 47.19212341308594\n",
      "[0515/1000] - train loss: 24.271875381469727, valid loss: 47.121307373046875\n",
      "[0516/1000] - train loss: 24.230777740478516, valid loss: 47.05093002319336\n",
      "[0517/1000] - train loss: 24.189952850341797, valid loss: 46.98099136352539\n",
      "[0518/1000] - train loss: 24.149412155151367, valid loss: 46.911285400390625\n",
      "[0519/1000] - train loss: 24.1091251373291, valid loss: 46.841827392578125\n",
      "[0520/1000] - train loss: 24.069107055664062, valid loss: 46.77263641357422\n",
      "[0521/1000] - train loss: 24.02935028076172, valid loss: 46.7037239074707\n",
      "[0522/1000] - train loss: 23.989871978759766, valid loss: 46.63508605957031\n",
      "[0523/1000] - train loss: 23.950721740722656, valid loss: 46.566768646240234\n",
      "[0524/1000] - train loss: 23.911827087402344, valid loss: 46.498779296875\n",
      "[0525/1000] - train loss: 23.87320327758789, valid loss: 46.431175231933594\n",
      "[0526/1000] - train loss: 23.834840774536133, valid loss: 46.36397933959961\n",
      "[0527/1000] - train loss: 23.796701431274414, valid loss: 46.29741287231445\n",
      "[0528/1000] - train loss: 23.758777618408203, valid loss: 46.231895446777344\n",
      "[0529/1000] - train loss: 23.72108268737793, valid loss: 46.166893005371094\n",
      "[0530/1000] - train loss: 23.683612823486328, valid loss: 46.102317810058594\n",
      "[0531/1000] - train loss: 23.646329879760742, valid loss: 46.03819274902344\n",
      "[0532/1000] - train loss: 23.609268188476562, valid loss: 45.97449493408203\n",
      "[0533/1000] - train loss: 23.572429656982422, valid loss: 45.910850524902344\n",
      "[0534/1000] - train loss: 23.535808563232422, valid loss: 45.846492767333984\n",
      "[0535/1000] - train loss: 23.499404907226562, valid loss: 45.78230285644531\n",
      "[0536/1000] - train loss: 23.463214874267578, valid loss: 45.7183952331543\n",
      "[0537/1000] - train loss: 23.427165985107422, valid loss: 45.6547737121582\n",
      "[0538/1000] - train loss: 23.391292572021484, valid loss: 45.59162139892578\n",
      "[0539/1000] - train loss: 23.355426788330078, valid loss: 45.52877426147461\n",
      "[0540/1000] - train loss: 23.31977081298828, valid loss: 45.46617889404297\n",
      "[0541/1000] - train loss: 23.28436279296875, valid loss: 45.40385818481445\n",
      "[0542/1000] - train loss: 23.24913787841797, valid loss: 45.341156005859375\n",
      "[0543/1000] - train loss: 23.21409034729004, valid loss: 45.27877426147461\n",
      "[0544/1000] - train loss: 23.179224014282227, valid loss: 45.21642303466797\n",
      "[0545/1000] - train loss: 23.14457893371582, valid loss: 45.154571533203125\n",
      "[0546/1000] - train loss: 23.110231399536133, valid loss: 45.092864990234375\n",
      "[0547/1000] - train loss: 23.076068878173828, valid loss: 45.031307220458984\n",
      "[0548/1000] - train loss: 23.042085647583008, valid loss: 44.970157623291016\n",
      "[0549/1000] - train loss: 23.008291244506836, valid loss: 44.909423828125\n",
      "[0550/1000] - train loss: 22.974689483642578, valid loss: 44.84904098510742\n",
      "[0551/1000] - train loss: 22.941261291503906, valid loss: 44.789039611816406\n",
      "[0552/1000] - train loss: 22.907949447631836, valid loss: 44.72943115234375\n",
      "[0553/1000] - train loss: 22.874813079833984, valid loss: 44.67008590698242\n",
      "[0554/1000] - train loss: 22.841842651367188, valid loss: 44.61100769042969\n",
      "[0555/1000] - train loss: 22.809038162231445, valid loss: 44.552284240722656\n",
      "[0556/1000] - train loss: 22.77637481689453, valid loss: 44.49400329589844\n",
      "[0557/1000] - train loss: 22.743812561035156, valid loss: 44.43617248535156\n",
      "[0558/1000] - train loss: 22.71139907836914, valid loss: 44.378787994384766\n",
      "[0559/1000] - train loss: 22.679134368896484, valid loss: 44.32184982299805\n",
      "[0560/1000] - train loss: 22.64703369140625, valid loss: 44.26533126831055\n",
      "[0561/1000] - train loss: 22.615087509155273, valid loss: 44.20891571044922\n",
      "[0562/1000] - train loss: 22.58330535888672, valid loss: 44.15288162231445\n",
      "[0563/1000] - train loss: 22.551687240600586, valid loss: 44.097190856933594\n",
      "[0564/1000] - train loss: 22.520221710205078, valid loss: 44.04184341430664\n",
      "[0565/1000] - train loss: 22.488910675048828, valid loss: 43.98681640625\n",
      "[0566/1000] - train loss: 22.457685470581055, valid loss: 43.932132720947266\n",
      "[0567/1000] - train loss: 22.42656707763672, valid loss: 43.87779998779297\n",
      "[0568/1000] - train loss: 22.39553451538086, valid loss: 43.82378005981445\n",
      "[0569/1000] - train loss: 22.364654541015625, valid loss: 43.77009201049805\n",
      "[0570/1000] - train loss: 22.33390998840332, valid loss: 43.716522216796875\n",
      "[0571/1000] - train loss: 22.30328941345215, valid loss: 43.66323471069336\n",
      "[0572/1000] - train loss: 22.272808074951172, valid loss: 43.61025619506836\n",
      "[0573/1000] - train loss: 22.242431640625, valid loss: 43.55759811401367\n",
      "[0574/1000] - train loss: 22.212173461914062, valid loss: 43.50524139404297\n",
      "[0575/1000] - train loss: 22.182044982910156, valid loss: 43.45317840576172\n",
      "[0576/1000] - train loss: 22.152015686035156, valid loss: 43.40143966674805\n",
      "[0577/1000] - train loss: 22.1220703125, valid loss: 43.349998474121094\n",
      "[0578/1000] - train loss: 22.092247009277344, valid loss: 43.29884338378906\n",
      "[0579/1000] - train loss: 22.062543869018555, valid loss: 43.24795150756836\n",
      "[0580/1000] - train loss: 22.032958984375, valid loss: 43.19730758666992\n",
      "[0581/1000] - train loss: 22.003496170043945, valid loss: 43.14689636230469\n",
      "[0582/1000] - train loss: 21.974149703979492, valid loss: 43.09673309326172\n",
      "[0583/1000] - train loss: 21.94490623474121, valid loss: 43.04681396484375\n",
      "[0584/1000] - train loss: 21.915769577026367, valid loss: 42.99711990356445\n",
      "[0585/1000] - train loss: 21.88674545288086, valid loss: 42.9476432800293\n",
      "[0586/1000] - train loss: 21.857837677001953, valid loss: 42.89838790893555\n",
      "[0587/1000] - train loss: 21.829044342041016, valid loss: 42.84933090209961\n",
      "[0588/1000] - train loss: 21.80035972595215, valid loss: 42.80047607421875\n",
      "[0589/1000] - train loss: 21.771793365478516, valid loss: 42.751808166503906\n",
      "[0590/1000] - train loss: 21.743343353271484, valid loss: 42.70284652709961\n",
      "[0591/1000] - train loss: 21.71497917175293, valid loss: 42.65391540527344\n",
      "[0592/1000] - train loss: 21.686723709106445, valid loss: 42.60519027709961\n",
      "[0593/1000] - train loss: 21.658573150634766, valid loss: 42.556678771972656\n",
      "[0594/1000] - train loss: 21.63047981262207, valid loss: 42.50838088989258\n",
      "[0595/1000] - train loss: 21.60246467590332, valid loss: 42.460235595703125\n",
      "[0596/1000] - train loss: 21.57455062866211, valid loss: 42.41222381591797\n",
      "[0597/1000] - train loss: 21.546730041503906, valid loss: 42.36441421508789\n",
      "[0598/1000] - train loss: 21.519012451171875, valid loss: 42.3167839050293\n",
      "[0599/1000] - train loss: 21.49139404296875, valid loss: 42.26921081542969\n",
      "[0600/1000] - train loss: 21.4638729095459, valid loss: 42.22169876098633\n",
      "[0601/1000] - train loss: 21.436450958251953, valid loss: 42.174354553222656\n",
      "[0602/1000] - train loss: 21.40912437438965, valid loss: 42.127159118652344\n",
      "[0603/1000] - train loss: 21.381893157958984, valid loss: 42.080116271972656\n",
      "[0604/1000] - train loss: 21.35476303100586, valid loss: 42.03321838378906\n",
      "[0605/1000] - train loss: 21.32772445678711, valid loss: 41.98647689819336\n",
      "[0606/1000] - train loss: 21.300779342651367, valid loss: 41.93987274169922\n",
      "[0607/1000] - train loss: 21.273941040039062, valid loss: 41.89350128173828\n",
      "[0608/1000] - train loss: 21.247196197509766, valid loss: 41.84728240966797\n",
      "[0609/1000] - train loss: 21.220550537109375, valid loss: 41.80123519897461\n",
      "[0610/1000] - train loss: 21.194000244140625, valid loss: 41.75537109375\n",
      "[0611/1000] - train loss: 21.167564392089844, valid loss: 41.709693908691406\n",
      "[0612/1000] - train loss: 21.141237258911133, valid loss: 41.6641960144043\n",
      "[0613/1000] - train loss: 21.114992141723633, valid loss: 41.6188850402832\n",
      "[0614/1000] - train loss: 21.088804244995117, valid loss: 41.57376480102539\n",
      "[0615/1000] - train loss: 21.062700271606445, valid loss: 41.5288200378418\n",
      "[0616/1000] - train loss: 21.03664779663086, valid loss: 41.48406982421875\n",
      "[0617/1000] - train loss: 21.010652542114258, valid loss: 41.43950271606445\n",
      "[0618/1000] - train loss: 20.984798431396484, valid loss: 41.39512634277344\n",
      "[0619/1000] - train loss: 20.959163665771484, valid loss: 41.35093688964844\n",
      "[0620/1000] - train loss: 20.933603286743164, valid loss: 41.306922912597656\n",
      "[0621/1000] - train loss: 20.90796661376953, valid loss: 41.26312255859375\n",
      "[0622/1000] - train loss: 20.882394790649414, valid loss: 41.219749450683594\n",
      "[0623/1000] - train loss: 20.856853485107422, valid loss: 41.17656707763672\n",
      "[0624/1000] - train loss: 20.83129119873047, valid loss: 41.1335563659668\n",
      "[0625/1000] - train loss: 20.805782318115234, valid loss: 41.09070587158203\n",
      "[0626/1000] - train loss: 20.780290603637695, valid loss: 41.04804229736328\n",
      "[0627/1000] - train loss: 20.754863739013672, valid loss: 41.00553512573242\n",
      "[0628/1000] - train loss: 20.7294979095459, valid loss: 40.96318817138672\n",
      "[0629/1000] - train loss: 20.70418930053711, valid loss: 40.92107009887695\n",
      "[0630/1000] - train loss: 20.67893409729004, valid loss: 40.87910842895508\n",
      "[0631/1000] - train loss: 20.653730392456055, valid loss: 40.83727264404297\n",
      "[0632/1000] - train loss: 20.628597259521484, valid loss: 40.79561233520508\n",
      "[0633/1000] - train loss: 20.603525161743164, valid loss: 40.75411605834961\n",
      "[0634/1000] - train loss: 20.578516006469727, valid loss: 40.71266174316406\n",
      "[0635/1000] - train loss: 20.55356788635254, valid loss: 40.67119598388672\n",
      "[0636/1000] - train loss: 20.528675079345703, valid loss: 40.62975311279297\n",
      "[0637/1000] - train loss: 20.50385284423828, valid loss: 40.58845901489258\n",
      "[0638/1000] - train loss: 20.479084014892578, valid loss: 40.54732131958008\n",
      "[0639/1000] - train loss: 20.454374313354492, valid loss: 40.506324768066406\n",
      "[0640/1000] - train loss: 20.429704666137695, valid loss: 40.4654655456543\n",
      "[0641/1000] - train loss: 20.40505027770996, valid loss: 40.42473602294922\n",
      "[0642/1000] - train loss: 20.380449295043945, valid loss: 40.38414001464844\n",
      "[0643/1000] - train loss: 20.355894088745117, valid loss: 40.34367370605469\n",
      "[0644/1000] - train loss: 20.331390380859375, valid loss: 40.30335235595703\n",
      "[0645/1000] - train loss: 20.306936264038086, valid loss: 40.26316452026367\n",
      "[0646/1000] - train loss: 20.282533645629883, valid loss: 40.22311019897461\n",
      "[0647/1000] - train loss: 20.258203506469727, valid loss: 40.183197021484375\n",
      "[0648/1000] - train loss: 20.233930587768555, valid loss: 40.143341064453125\n",
      "[0649/1000] - train loss: 20.209728240966797, valid loss: 40.10356140136719\n",
      "[0650/1000] - train loss: 20.185585021972656, valid loss: 40.06385803222656\n",
      "[0651/1000] - train loss: 20.16149139404297, valid loss: 40.02423858642578\n",
      "[0652/1000] - train loss: 20.1374568939209, valid loss: 39.98470687866211\n",
      "[0653/1000] - train loss: 20.11351203918457, valid loss: 39.94514846801758\n",
      "[0654/1000] - train loss: 20.08963394165039, valid loss: 39.90557861328125\n",
      "[0655/1000] - train loss: 20.065813064575195, valid loss: 39.86601638793945\n",
      "[0656/1000] - train loss: 20.042072296142578, valid loss: 39.82646942138672\n",
      "[0657/1000] - train loss: 20.018396377563477, valid loss: 39.78695297241211\n",
      "[0658/1000] - train loss: 19.994779586791992, valid loss: 39.74748992919922\n",
      "[0659/1000] - train loss: 19.971229553222656, valid loss: 39.70808792114258\n",
      "[0660/1000] - train loss: 19.947744369506836, valid loss: 39.66876983642578\n",
      "[0661/1000] - train loss: 19.924318313598633, valid loss: 39.6295280456543\n",
      "[0662/1000] - train loss: 19.90095329284668, valid loss: 39.59039306640625\n",
      "[0663/1000] - train loss: 19.877643585205078, valid loss: 39.551368713378906\n",
      "[0664/1000] - train loss: 19.854389190673828, valid loss: 39.512454986572266\n",
      "[0665/1000] - train loss: 19.831192016601562, valid loss: 39.47367477416992\n",
      "[0666/1000] - train loss: 19.808053970336914, valid loss: 39.43510818481445\n",
      "[0667/1000] - train loss: 19.784976959228516, valid loss: 39.396751403808594\n",
      "[0668/1000] - train loss: 19.761966705322266, valid loss: 39.35862731933594\n",
      "[0669/1000] - train loss: 19.7390079498291, valid loss: 39.320716857910156\n",
      "[0670/1000] - train loss: 19.716102600097656, valid loss: 39.28302764892578\n",
      "[0671/1000] - train loss: 19.693254470825195, valid loss: 39.24554443359375\n",
      "[0672/1000] - train loss: 19.670455932617188, valid loss: 39.208274841308594\n",
      "[0673/1000] - train loss: 19.647703170776367, valid loss: 39.171226501464844\n",
      "[0674/1000] - train loss: 19.624969482421875, valid loss: 39.13439178466797\n",
      "[0675/1000] - train loss: 19.60224151611328, valid loss: 39.09775924682617\n",
      "[0676/1000] - train loss: 19.57952308654785, valid loss: 39.06133270263672\n",
      "[0677/1000] - train loss: 19.55683708190918, valid loss: 39.02509307861328\n",
      "[0678/1000] - train loss: 19.534196853637695, valid loss: 38.98906326293945\n",
      "[0679/1000] - train loss: 19.511594772338867, valid loss: 38.9532356262207\n",
      "[0680/1000] - train loss: 19.489046096801758, valid loss: 38.917579650878906\n",
      "[0681/1000] - train loss: 19.466541290283203, valid loss: 38.882102966308594\n",
      "[0682/1000] - train loss: 19.444072723388672, valid loss: 38.846797943115234\n",
      "[0683/1000] - train loss: 19.4216365814209, valid loss: 38.811649322509766\n",
      "[0684/1000] - train loss: 19.39923667907715, valid loss: 38.776668548583984\n",
      "[0685/1000] - train loss: 19.376882553100586, valid loss: 38.74184799194336\n",
      "[0686/1000] - train loss: 19.35456657409668, valid loss: 38.7071647644043\n",
      "[0687/1000] - train loss: 19.332290649414062, valid loss: 38.672630310058594\n",
      "[0688/1000] - train loss: 19.310033798217773, valid loss: 38.638214111328125\n",
      "[0689/1000] - train loss: 19.287784576416016, valid loss: 38.603721618652344\n",
      "[0690/1000] - train loss: 19.265748977661133, valid loss: 38.56916046142578\n",
      "[0691/1000] - train loss: 19.24373435974121, valid loss: 38.53455352783203\n",
      "[0692/1000] - train loss: 19.22174644470215, valid loss: 38.49991226196289\n",
      "[0693/1000] - train loss: 19.199790954589844, valid loss: 38.46525955200195\n",
      "[0694/1000] - train loss: 19.17786407470703, valid loss: 38.430606842041016\n",
      "[0695/1000] - train loss: 19.155975341796875, valid loss: 38.39598083496094\n",
      "[0696/1000] - train loss: 19.13412094116211, valid loss: 38.36137390136719\n",
      "[0697/1000] - train loss: 19.112302780151367, valid loss: 38.326786041259766\n",
      "[0698/1000] - train loss: 19.09052848815918, valid loss: 38.2922477722168\n",
      "[0699/1000] - train loss: 19.068790435791016, valid loss: 38.25775146484375\n",
      "[0700/1000] - train loss: 19.04709243774414, valid loss: 38.22332763671875\n",
      "[0701/1000] - train loss: 19.025392532348633, valid loss: 38.1889533996582\n",
      "[0702/1000] - train loss: 19.003705978393555, valid loss: 38.154640197753906\n",
      "[0703/1000] - train loss: 18.982030868530273, valid loss: 38.12042999267578\n",
      "[0704/1000] - train loss: 18.960351943969727, valid loss: 38.086326599121094\n",
      "[0705/1000] - train loss: 18.938718795776367, valid loss: 38.05233383178711\n",
      "[0706/1000] - train loss: 18.917179107666016, valid loss: 38.01861572265625\n",
      "[0707/1000] - train loss: 18.89568519592285, valid loss: 37.985172271728516\n",
      "[0708/1000] - train loss: 18.87423324584961, valid loss: 37.951927185058594\n",
      "[0709/1000] - train loss: 18.852802276611328, valid loss: 37.91876220703125\n",
      "[0710/1000] - train loss: 18.831401824951172, valid loss: 37.88570022583008\n",
      "[0711/1000] - train loss: 18.81002426147461, valid loss: 37.852752685546875\n",
      "[0712/1000] - train loss: 18.78866958618164, valid loss: 37.82000732421875\n",
      "[0713/1000] - train loss: 18.76734161376953, valid loss: 37.7874755859375\n",
      "[0714/1000] - train loss: 18.746061325073242, valid loss: 37.75492477416992\n",
      "[0715/1000] - train loss: 18.724882125854492, valid loss: 37.722286224365234\n",
      "[0716/1000] - train loss: 18.7037296295166, valid loss: 37.689666748046875\n",
      "[0717/1000] - train loss: 18.6826114654541, valid loss: 37.65706253051758\n",
      "[0718/1000] - train loss: 18.661523818969727, valid loss: 37.62448501586914\n",
      "[0719/1000] - train loss: 18.64046859741211, valid loss: 37.59193801879883\n",
      "[0720/1000] - train loss: 18.619449615478516, valid loss: 37.559600830078125\n",
      "[0721/1000] - train loss: 18.598482131958008, valid loss: 37.52763748168945\n",
      "[0722/1000] - train loss: 18.57756996154785, valid loss: 37.496009826660156\n",
      "[0723/1000] - train loss: 18.556705474853516, valid loss: 37.46480178833008\n",
      "[0724/1000] - train loss: 18.535873413085938, valid loss: 37.43370056152344\n",
      "[0725/1000] - train loss: 18.515174865722656, valid loss: 37.40223693847656\n",
      "[0726/1000] - train loss: 18.494558334350586, valid loss: 37.370357513427734\n",
      "[0727/1000] - train loss: 18.473974227905273, valid loss: 37.33846664428711\n",
      "[0728/1000] - train loss: 18.45342445373535, valid loss: 37.30656051635742\n",
      "[0729/1000] - train loss: 18.432880401611328, valid loss: 37.27462387084961\n",
      "[0730/1000] - train loss: 18.41226577758789, valid loss: 37.242801666259766\n",
      "[0731/1000] - train loss: 18.39166259765625, valid loss: 37.210914611816406\n",
      "[0732/1000] - train loss: 18.371061325073242, valid loss: 37.179168701171875\n",
      "[0733/1000] - train loss: 18.35050392150879, valid loss: 37.147918701171875\n",
      "[0734/1000] - train loss: 18.3299617767334, valid loss: 37.11676788330078\n",
      "[0735/1000] - train loss: 18.30943489074707, valid loss: 37.085758209228516\n",
      "[0736/1000] - train loss: 18.288925170898438, valid loss: 37.05466842651367\n",
      "[0737/1000] - train loss: 18.268428802490234, valid loss: 37.02280044555664\n",
      "[0738/1000] - train loss: 18.247926712036133, valid loss: 36.99087905883789\n",
      "[0739/1000] - train loss: 18.227462768554688, valid loss: 36.959110260009766\n",
      "[0740/1000] - train loss: 18.20697021484375, valid loss: 36.927642822265625\n",
      "[0741/1000] - train loss: 18.186487197875977, valid loss: 36.89643478393555\n",
      "[0742/1000] - train loss: 18.165996551513672, valid loss: 36.86506652832031\n",
      "[0743/1000] - train loss: 18.14554214477539, valid loss: 36.833438873291016\n",
      "[0744/1000] - train loss: 18.1251163482666, valid loss: 36.801692962646484\n",
      "[0745/1000] - train loss: 18.10471534729004, valid loss: 36.770050048828125\n",
      "[0746/1000] - train loss: 18.084362030029297, valid loss: 36.73843002319336\n",
      "[0747/1000] - train loss: 18.064056396484375, valid loss: 36.70703887939453\n",
      "[0748/1000] - train loss: 18.043853759765625, valid loss: 36.67550277709961\n",
      "[0749/1000] - train loss: 18.023725509643555, valid loss: 36.64341354370117\n",
      "[0750/1000] - train loss: 18.003631591796875, valid loss: 36.61115264892578\n",
      "[0751/1000] - train loss: 17.983549118041992, valid loss: 36.579078674316406\n",
      "[0752/1000] - train loss: 17.963499069213867, valid loss: 36.54718017578125\n",
      "[0753/1000] - train loss: 17.943477630615234, valid loss: 36.515045166015625\n",
      "[0754/1000] - train loss: 17.923503875732422, valid loss: 36.48290252685547\n",
      "[0755/1000] - train loss: 17.9035587310791, valid loss: 36.450469970703125\n",
      "[0756/1000] - train loss: 17.883691787719727, valid loss: 36.4182014465332\n",
      "[0757/1000] - train loss: 17.863862991333008, valid loss: 36.385799407958984\n",
      "[0758/1000] - train loss: 17.84406852722168, valid loss: 36.35314178466797\n",
      "[0759/1000] - train loss: 17.824316024780273, valid loss: 36.320125579833984\n",
      "[0760/1000] - train loss: 17.80454444885254, valid loss: 36.28719711303711\n",
      "[0761/1000] - train loss: 17.784805297851562, valid loss: 36.254356384277344\n",
      "[0762/1000] - train loss: 17.765094757080078, valid loss: 36.22184753417969\n",
      "[0763/1000] - train loss: 17.74544334411621, valid loss: 36.18914794921875\n",
      "[0764/1000] - train loss: 17.725812911987305, valid loss: 36.15638732910156\n",
      "[0765/1000] - train loss: 17.706188201904297, valid loss: 36.12380599975586\n",
      "[0766/1000] - train loss: 17.686616897583008, valid loss: 36.09138107299805\n",
      "[0767/1000] - train loss: 17.66705894470215, valid loss: 36.05910873413086\n",
      "[0768/1000] - train loss: 17.64751434326172, valid loss: 36.027217864990234\n",
      "[0769/1000] - train loss: 17.62799072265625, valid loss: 35.99568176269531\n",
      "[0770/1000] - train loss: 17.608476638793945, valid loss: 35.96410369873047\n",
      "[0771/1000] - train loss: 17.589014053344727, valid loss: 35.93230056762695\n",
      "[0772/1000] - train loss: 17.569578170776367, valid loss: 35.90047073364258\n",
      "[0773/1000] - train loss: 17.550113677978516, valid loss: 35.86883544921875\n",
      "[0774/1000] - train loss: 17.5306453704834, valid loss: 35.83740997314453\n",
      "[0775/1000] - train loss: 17.511213302612305, valid loss: 35.806373596191406\n",
      "[0776/1000] - train loss: 17.4918155670166, valid loss: 35.775482177734375\n",
      "[0777/1000] - train loss: 17.472434997558594, valid loss: 35.744728088378906\n",
      "[0778/1000] - train loss: 17.453075408935547, valid loss: 35.714115142822266\n",
      "[0779/1000] - train loss: 17.433773040771484, valid loss: 35.683555603027344\n",
      "[0780/1000] - train loss: 17.414499282836914, valid loss: 35.65276336669922\n",
      "[0781/1000] - train loss: 17.395292282104492, valid loss: 35.621925354003906\n",
      "[0782/1000] - train loss: 17.376218795776367, valid loss: 35.59235763549805\n",
      "[0783/1000] - train loss: 17.357402801513672, valid loss: 35.56415557861328\n",
      "[0784/1000] - train loss: 17.338577270507812, valid loss: 35.53694152832031\n",
      "[0785/1000] - train loss: 17.319747924804688, valid loss: 35.510562896728516\n",
      "[0786/1000] - train loss: 17.300914764404297, valid loss: 35.484745025634766\n",
      "[0787/1000] - train loss: 17.28200340270996, valid loss: 35.459510803222656\n",
      "[0788/1000] - train loss: 17.263051986694336, valid loss: 35.434574127197266\n",
      "[0789/1000] - train loss: 17.244096755981445, valid loss: 35.410308837890625\n",
      "[0790/1000] - train loss: 17.225122451782227, valid loss: 35.38682556152344\n",
      "[0791/1000] - train loss: 17.206148147583008, valid loss: 35.3637809753418\n",
      "[0792/1000] - train loss: 17.18718147277832, valid loss: 35.34089660644531\n",
      "[0793/1000] - train loss: 17.168291091918945, valid loss: 35.317962646484375\n",
      "[0794/1000] - train loss: 17.14944839477539, valid loss: 35.29495620727539\n",
      "[0795/1000] - train loss: 17.130659103393555, valid loss: 35.27206039428711\n",
      "[0796/1000] - train loss: 17.111913681030273, valid loss: 35.24922180175781\n",
      "[0797/1000] - train loss: 17.093177795410156, valid loss: 35.22639846801758\n",
      "[0798/1000] - train loss: 17.07455825805664, valid loss: 35.20195770263672\n",
      "[0799/1000] - train loss: 17.056015014648438, valid loss: 35.17604446411133\n",
      "[0800/1000] - train loss: 17.037460327148438, valid loss: 35.148704528808594\n",
      "[0801/1000] - train loss: 17.01890754699707, valid loss: 35.12004470825195\n",
      "[0802/1000] - train loss: 17.0003719329834, valid loss: 35.09178924560547\n",
      "[0803/1000] - train loss: 16.981992721557617, valid loss: 35.06369400024414\n",
      "[0804/1000] - train loss: 16.963623046875, valid loss: 35.035648345947266\n",
      "[0805/1000] - train loss: 16.94525909423828, valid loss: 35.00786209106445\n",
      "[0806/1000] - train loss: 16.926912307739258, valid loss: 34.98044967651367\n",
      "[0807/1000] - train loss: 16.908483505249023, valid loss: 34.9531364440918\n",
      "[0808/1000] - train loss: 16.890071868896484, valid loss: 34.92609405517578\n",
      "[0809/1000] - train loss: 16.87166976928711, valid loss: 34.89912796020508\n",
      "[0810/1000] - train loss: 16.853229522705078, valid loss: 34.87236785888672\n",
      "[0811/1000] - train loss: 16.83478355407715, valid loss: 34.845359802246094\n",
      "[0812/1000] - train loss: 16.816356658935547, valid loss: 34.818416595458984\n",
      "[0813/1000] - train loss: 16.79791831970215, valid loss: 34.79151916503906\n",
      "[0814/1000] - train loss: 16.779504776000977, valid loss: 34.76301956176758\n",
      "[0815/1000] - train loss: 16.761014938354492, valid loss: 34.733089447021484\n",
      "[0816/1000] - train loss: 16.74254608154297, valid loss: 34.70355224609375\n",
      "[0817/1000] - train loss: 16.724102020263672, valid loss: 34.674583435058594\n",
      "[0818/1000] - train loss: 16.705646514892578, valid loss: 34.646148681640625\n",
      "[0819/1000] - train loss: 16.68720054626465, valid loss: 34.61798095703125\n",
      "[0820/1000] - train loss: 16.66876983642578, valid loss: 34.5900764465332\n",
      "[0821/1000] - train loss: 16.650365829467773, valid loss: 34.562644958496094\n",
      "[0822/1000] - train loss: 16.631973266601562, valid loss: 34.535648345947266\n",
      "[0823/1000] - train loss: 16.613595962524414, valid loss: 34.50885009765625\n",
      "[0824/1000] - train loss: 16.59526252746582, valid loss: 34.482025146484375\n",
      "[0825/1000] - train loss: 16.576936721801758, valid loss: 34.45514678955078\n",
      "[0826/1000] - train loss: 16.558574676513672, valid loss: 34.42826461791992\n",
      "[0827/1000] - train loss: 16.540225982666016, valid loss: 34.4017448425293\n",
      "[0828/1000] - train loss: 16.521940231323242, valid loss: 34.374027252197266\n",
      "[0829/1000] - train loss: 16.50359535217285, valid loss: 34.34530258178711\n",
      "[0830/1000] - train loss: 16.48534393310547, valid loss: 34.31720733642578\n",
      "[0831/1000] - train loss: 16.467124938964844, valid loss: 34.28963088989258\n",
      "[0832/1000] - train loss: 16.448972702026367, valid loss: 34.26222610473633\n",
      "[0833/1000] - train loss: 16.430870056152344, valid loss: 34.23513412475586\n",
      "[0834/1000] - train loss: 16.412654876708984, valid loss: 34.20831298828125\n",
      "[0835/1000] - train loss: 16.394412994384766, valid loss: 34.181976318359375\n",
      "[0836/1000] - train loss: 16.376201629638672, valid loss: 34.15608596801758\n",
      "[0837/1000] - train loss: 16.358001708984375, valid loss: 34.13039016723633\n",
      "[0838/1000] - train loss: 16.339832305908203, valid loss: 34.104862213134766\n",
      "[0839/1000] - train loss: 16.32169532775879, valid loss: 34.079261779785156\n",
      "[0840/1000] - train loss: 16.30357551574707, valid loss: 34.053585052490234\n",
      "[0841/1000] - train loss: 16.28546142578125, valid loss: 34.028072357177734\n",
      "[0842/1000] - train loss: 16.26738739013672, valid loss: 34.002708435058594\n",
      "[0843/1000] - train loss: 16.24938201904297, valid loss: 33.97593307495117\n",
      "[0844/1000] - train loss: 16.231307983398438, valid loss: 33.947845458984375\n",
      "[0845/1000] - train loss: 16.213281631469727, valid loss: 33.920166015625\n",
      "[0846/1000] - train loss: 16.195301055908203, valid loss: 33.89310836791992\n",
      "[0847/1000] - train loss: 16.177343368530273, valid loss: 33.86663055419922\n",
      "[0848/1000] - train loss: 16.15937042236328, valid loss: 33.8407096862793\n",
      "[0849/1000] - train loss: 16.141408920288086, valid loss: 33.815303802490234\n",
      "[0850/1000] - train loss: 16.123470306396484, valid loss: 33.79012680053711\n",
      "[0851/1000] - train loss: 16.105562210083008, valid loss: 33.765159606933594\n",
      "[0852/1000] - train loss: 16.08767318725586, valid loss: 33.7406120300293\n",
      "[0853/1000] - train loss: 16.069799423217773, valid loss: 33.7164421081543\n",
      "[0854/1000] - train loss: 16.051918029785156, valid loss: 33.69233322143555\n",
      "[0855/1000] - train loss: 16.03404998779297, valid loss: 33.66825866699219\n",
      "[0856/1000] - train loss: 16.016162872314453, valid loss: 33.64399719238281\n",
      "[0857/1000] - train loss: 15.998247146606445, valid loss: 33.61958694458008\n",
      "[0858/1000] - train loss: 15.980287551879883, valid loss: 33.59501647949219\n",
      "[0859/1000] - train loss: 15.962376594543457, valid loss: 33.570552825927734\n",
      "[0860/1000] - train loss: 15.944491386413574, valid loss: 33.546119689941406\n",
      "[0861/1000] - train loss: 15.926616668701172, valid loss: 33.52192306518555\n",
      "[0862/1000] - train loss: 15.908764839172363, valid loss: 33.49798583984375\n",
      "[0863/1000] - train loss: 15.890935897827148, valid loss: 33.4726448059082\n",
      "[0864/1000] - train loss: 15.873114585876465, valid loss: 33.44769287109375\n",
      "[0865/1000] - train loss: 15.85532283782959, valid loss: 33.422855377197266\n",
      "[0866/1000] - train loss: 15.837553024291992, valid loss: 33.39812469482422\n",
      "[0867/1000] - train loss: 15.819785118103027, valid loss: 33.37372970581055\n",
      "[0868/1000] - train loss: 15.802057266235352, valid loss: 33.34943389892578\n",
      "[0869/1000] - train loss: 15.784341812133789, valid loss: 33.32521057128906\n",
      "[0870/1000] - train loss: 15.766669273376465, valid loss: 33.30082321166992\n",
      "[0871/1000] - train loss: 15.749002456665039, valid loss: 33.27626037597656\n",
      "[0872/1000] - train loss: 15.731376647949219, valid loss: 33.252052307128906\n",
      "[0873/1000] - train loss: 15.71375560760498, valid loss: 33.22810745239258\n",
      "[0874/1000] - train loss: 15.695962905883789, valid loss: 33.204463958740234\n",
      "[0875/1000] - train loss: 15.678176879882812, valid loss: 33.181034088134766\n",
      "[0876/1000] - train loss: 15.660396575927734, valid loss: 33.15755844116211\n",
      "[0877/1000] - train loss: 15.642633438110352, valid loss: 33.13359832763672\n",
      "[0878/1000] - train loss: 15.624883651733398, valid loss: 33.109352111816406\n",
      "[0879/1000] - train loss: 15.607170104980469, valid loss: 33.08533477783203\n",
      "[0880/1000] - train loss: 15.589555740356445, valid loss: 33.0615348815918\n",
      "[0881/1000] - train loss: 15.57197093963623, valid loss: 33.037940979003906\n",
      "[0882/1000] - train loss: 15.554411888122559, valid loss: 33.01454544067383\n",
      "[0883/1000] - train loss: 15.53689193725586, valid loss: 32.99068832397461\n",
      "[0884/1000] - train loss: 15.51938533782959, valid loss: 32.96649932861328\n",
      "[0885/1000] - train loss: 15.501895904541016, valid loss: 32.9426155090332\n",
      "[0886/1000] - train loss: 15.484420776367188, valid loss: 32.919002532958984\n",
      "[0887/1000] - train loss: 15.466976165771484, valid loss: 32.895633697509766\n",
      "[0888/1000] - train loss: 15.449554443359375, valid loss: 32.870479583740234\n",
      "[0889/1000] - train loss: 15.432147026062012, valid loss: 32.84563064575195\n",
      "[0890/1000] - train loss: 15.414799690246582, valid loss: 32.82111740112305\n",
      "[0891/1000] - train loss: 15.397539138793945, valid loss: 32.79664611816406\n",
      "[0892/1000] - train loss: 15.380325317382812, valid loss: 32.772193908691406\n",
      "[0893/1000] - train loss: 15.363158226013184, valid loss: 32.74776840209961\n",
      "[0894/1000] - train loss: 15.346070289611816, valid loss: 32.72364044189453\n",
      "[0895/1000] - train loss: 15.329007148742676, valid loss: 32.699520111083984\n",
      "[0896/1000] - train loss: 15.311969757080078, valid loss: 32.675418853759766\n",
      "[0897/1000] - train loss: 15.294958114624023, valid loss: 32.65131378173828\n",
      "[0898/1000] - train loss: 15.277976036071777, valid loss: 32.627437591552734\n",
      "[0899/1000] - train loss: 15.2610445022583, valid loss: 32.603797912597656\n",
      "[0900/1000] - train loss: 15.24413776397705, valid loss: 32.58036422729492\n",
      "[0901/1000] - train loss: 15.227259635925293, valid loss: 32.55710983276367\n",
      "[0902/1000] - train loss: 15.210415840148926, valid loss: 32.53401184082031\n",
      "[0903/1000] - train loss: 15.193653106689453, valid loss: 32.510799407958984\n",
      "[0904/1000] - train loss: 15.176905632019043, valid loss: 32.48720169067383\n",
      "[0905/1000] - train loss: 15.16018009185791, valid loss: 32.46327209472656\n",
      "[0906/1000] - train loss: 15.143482208251953, valid loss: 32.43928909301758\n",
      "[0907/1000] - train loss: 15.126861572265625, valid loss: 32.4155387878418\n",
      "[0908/1000] - train loss: 15.110278129577637, valid loss: 32.392032623291016\n",
      "[0909/1000] - train loss: 15.093721389770508, valid loss: 32.36874771118164\n",
      "[0910/1000] - train loss: 15.077193260192871, valid loss: 32.345664978027344\n",
      "[0911/1000] - train loss: 15.060694694519043, valid loss: 32.32276916503906\n",
      "[0912/1000] - train loss: 15.044230461120605, valid loss: 32.29976272583008\n",
      "[0913/1000] - train loss: 15.027804374694824, valid loss: 32.27692413330078\n",
      "[0914/1000] - train loss: 15.011427879333496, valid loss: 32.25369644165039\n",
      "[0915/1000] - train loss: 14.995083808898926, valid loss: 32.23039627075195\n",
      "[0916/1000] - train loss: 14.978767395019531, valid loss: 32.20704650878906\n",
      "[0917/1000] - train loss: 14.962482452392578, valid loss: 32.18391418457031\n",
      "[0918/1000] - train loss: 14.946224212646484, valid loss: 32.16099166870117\n",
      "[0919/1000] - train loss: 14.929993629455566, valid loss: 32.13826370239258\n",
      "[0920/1000] - train loss: 14.913809776306152, valid loss: 32.11543655395508\n",
      "[0921/1000] - train loss: 14.897624015808105, valid loss: 32.092525482177734\n",
      "[0922/1000] - train loss: 14.88145637512207, valid loss: 32.069820404052734\n",
      "[0923/1000] - train loss: 14.865339279174805, valid loss: 32.04719161987305\n",
      "[0924/1000] - train loss: 14.849266052246094, valid loss: 32.02448654174805\n",
      "[0925/1000] - train loss: 14.83324146270752, valid loss: 32.00154495239258\n",
      "[0926/1000] - train loss: 14.817251205444336, valid loss: 31.97835922241211\n",
      "[0927/1000] - train loss: 14.801283836364746, valid loss: 31.954994201660156\n",
      "[0928/1000] - train loss: 14.78542423248291, valid loss: 31.93147850036621\n",
      "[0929/1000] - train loss: 14.769577980041504, valid loss: 31.9078426361084\n",
      "[0930/1000] - train loss: 14.753762245178223, valid loss: 31.8843936920166\n",
      "[0931/1000] - train loss: 14.737996101379395, valid loss: 31.86115074157715\n",
      "[0932/1000] - train loss: 14.722249984741211, valid loss: 31.838096618652344\n",
      "[0933/1000] - train loss: 14.70654010772705, valid loss: 31.815326690673828\n",
      "[0934/1000] - train loss: 14.690858840942383, valid loss: 31.79268455505371\n",
      "[0935/1000] - train loss: 14.675215721130371, valid loss: 31.769878387451172\n",
      "[0936/1000] - train loss: 14.659637451171875, valid loss: 31.74665641784668\n",
      "[0937/1000] - train loss: 14.644079208374023, valid loss: 31.72336196899414\n",
      "[0938/1000] - train loss: 14.62856388092041, valid loss: 31.700326919555664\n",
      "[0939/1000] - train loss: 14.613090515136719, valid loss: 31.67753028869629\n",
      "[0940/1000] - train loss: 14.597651481628418, valid loss: 31.654956817626953\n",
      "[0941/1000] - train loss: 14.582240104675293, valid loss: 31.632293701171875\n",
      "[0942/1000] - train loss: 14.566858291625977, valid loss: 31.609600067138672\n",
      "[0943/1000] - train loss: 14.551495552062988, valid loss: 31.587173461914062\n",
      "[0944/1000] - train loss: 14.53617000579834, valid loss: 31.565017700195312\n",
      "[0945/1000] - train loss: 14.520913124084473, valid loss: 31.542861938476562\n",
      "[0946/1000] - train loss: 14.505492210388184, valid loss: 31.52082633972168\n",
      "[0947/1000] - train loss: 14.489943504333496, valid loss: 31.49922752380371\n",
      "[0948/1000] - train loss: 14.474430084228516, valid loss: 31.47774314880371\n",
      "[0949/1000] - train loss: 14.4589204788208, valid loss: 31.45635223388672\n",
      "[0950/1000] - train loss: 14.443428039550781, valid loss: 31.435346603393555\n",
      "[0951/1000] - train loss: 14.42795467376709, valid loss: 31.414670944213867\n",
      "[0952/1000] - train loss: 14.412497520446777, valid loss: 31.394102096557617\n",
      "[0953/1000] - train loss: 14.397080421447754, valid loss: 31.37350845336914\n",
      "[0954/1000] - train loss: 14.381638526916504, valid loss: 31.352630615234375\n",
      "[0955/1000] - train loss: 14.366251945495605, valid loss: 31.331762313842773\n",
      "[0956/1000] - train loss: 14.350871086120605, valid loss: 31.311168670654297\n",
      "[0957/1000] - train loss: 14.335466384887695, valid loss: 31.290821075439453\n",
      "[0958/1000] - train loss: 14.32007122039795, valid loss: 31.270727157592773\n",
      "[0959/1000] - train loss: 14.304591178894043, valid loss: 31.25111961364746\n",
      "[0960/1000] - train loss: 14.288966178894043, valid loss: 31.231943130493164\n",
      "[0961/1000] - train loss: 14.273336410522461, valid loss: 31.213125228881836\n",
      "[0962/1000] - train loss: 14.257749557495117, valid loss: 31.194263458251953\n",
      "[0963/1000] - train loss: 14.242223739624023, valid loss: 31.17562484741211\n",
      "[0964/1000] - train loss: 14.226707458496094, valid loss: 31.156505584716797\n",
      "[0965/1000] - train loss: 14.211212158203125, valid loss: 31.137313842773438\n",
      "[0966/1000] - train loss: 14.195712089538574, valid loss: 31.11813735961914\n",
      "[0967/1000] - train loss: 14.180130958557129, valid loss: 31.098970413208008\n",
      "[0968/1000] - train loss: 14.164555549621582, valid loss: 31.07980728149414\n",
      "[0969/1000] - train loss: 14.148988723754883, valid loss: 31.060640335083008\n",
      "[0970/1000] - train loss: 14.133464813232422, valid loss: 31.041488647460938\n",
      "[0971/1000] - train loss: 14.117980003356934, valid loss: 31.02205467224121\n",
      "[0972/1000] - train loss: 14.102490425109863, valid loss: 31.002695083618164\n",
      "[0973/1000] - train loss: 14.087031364440918, valid loss: 30.983291625976562\n",
      "[0974/1000] - train loss: 14.071600914001465, valid loss: 30.963790893554688\n",
      "[0975/1000] - train loss: 14.05620288848877, valid loss: 30.944196701049805\n",
      "[0976/1000] - train loss: 14.040828704833984, valid loss: 30.924694061279297\n",
      "[0977/1000] - train loss: 14.025464057922363, valid loss: 30.904869079589844\n",
      "[0978/1000] - train loss: 14.010122299194336, valid loss: 30.88503646850586\n",
      "[0979/1000] - train loss: 13.994806289672852, valid loss: 30.8655948638916\n",
      "[0980/1000] - train loss: 13.979501724243164, valid loss: 30.84639549255371\n",
      "[0981/1000] - train loss: 13.964229583740234, valid loss: 30.827213287353516\n",
      "[0982/1000] - train loss: 13.949041366577148, valid loss: 30.807645797729492\n",
      "[0983/1000] - train loss: 13.933780670166016, valid loss: 30.787744522094727\n",
      "[0984/1000] - train loss: 13.918521881103516, valid loss: 30.768251419067383\n",
      "[0985/1000] - train loss: 13.903310775756836, valid loss: 30.748716354370117\n",
      "[0986/1000] - train loss: 13.888120651245117, valid loss: 30.729137420654297\n",
      "[0987/1000] - train loss: 13.87295150756836, valid loss: 30.709516525268555\n",
      "[0988/1000] - train loss: 13.85780143737793, valid loss: 30.689857482910156\n",
      "[0989/1000] - train loss: 13.842677116394043, valid loss: 30.670568466186523\n",
      "[0990/1000] - train loss: 13.827571868896484, valid loss: 30.65130043029785\n",
      "[0991/1000] - train loss: 13.812491416931152, valid loss: 30.631837844848633\n",
      "[0992/1000] - train loss: 13.797423362731934, valid loss: 30.61208152770996\n",
      "[0993/1000] - train loss: 13.782402992248535, valid loss: 30.59215545654297\n",
      "[0994/1000] - train loss: 13.767329216003418, valid loss: 30.571840286254883\n",
      "[0995/1000] - train loss: 13.752212524414062, valid loss: 30.552169799804688\n",
      "[0996/1000] - train loss: 13.73708724975586, valid loss: 30.53266143798828\n",
      "[0997/1000] - train loss: 13.721944808959961, valid loss: 30.513320922851562\n",
      "[0998/1000] - train loss: 13.706790924072266, valid loss: 30.49413299560547\n",
      "[0999/1000] - train loss: 13.691658973693848, valid loss: 30.47536849975586\n",
      "[1000/1000] - train loss: 13.676599502563477, valid loss: 30.45657730102539\n",
      "총학습에 걸린시간(초): 3.792034864425659\n"
     ]
    }
   ],
   "source": [
    "####### Train + Evaluation(에폭당)\n",
    "import time\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs):  # tqdm 라이브러리\n",
    "    #######################################\n",
    "    # Train\n",
    "    #######################################\n",
    "    # 1. 모델을 train 모드로 변경\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0\n",
    "    # 2. Batch 단위로 학습 - 반복문.\n",
    "    for X_train, y_train in train_loader:\n",
    "        # 3. X, y를 device로 이동.\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # 4. 추론\n",
    "        pred = boston_model(X_train) # model.forward(X_train)\n",
    "        # 5. loss(오차) 계산\n",
    "        loss = loss_fn(pred, y_train)  # (추론한값,  정답)\n",
    "        # 6. 파라미터들의 gradient 계산\n",
    "        loss.backward()\n",
    "        # 7. 파라미터들 업데이트\n",
    "        optimizer.step()\n",
    "        # 8. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        ## 로그용 - LOSS 저장\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader) # train loss의 step별 평균을 계산 -> 한 epoch의 train loss\n",
    "\n",
    "    #######################################\n",
    "    # Evaluation\n",
    "    #######################################\n",
    "    # 1. eval 모드\n",
    "    boston_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    # 2. gradient function 구하지 않도록 처리\n",
    "    with torch.no_grad():\n",
    "        for X_valid, y_valid in test_loader: #step 단위로 평가\n",
    "            # 3. X, y를 device로 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "            # 4. 추론\n",
    "            pred_valid = boston_model(X_valid)\n",
    "            # 5. 검증(평가) - MSE\n",
    "            loss_valid = loss_fn(pred_valid, y_valid)\n",
    "            valid_loss += loss_valid.item()\n",
    "            \n",
    "        valid_loss /= len(test_loader) # 평균계산.\n",
    "\n",
    "    ## loss 계산한 것들을 list에 추가. 로그 출력\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f\"[{epoch+1:04d}/{epochs}] - train loss: {train_loss}, valid loss: {valid_loss}\")\n",
    "\n",
    "e = time.time()\n",
    "print(\"총학습에 걸린시간(초):\", (e-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss 시각화\n",
    "plt.plot(range(epochs), train_losses, label=\"train set\")\n",
    "plt.plot(range(epochs), valid_losses, label=\"valid set\")\n",
    "plt.title(\"학습 Loss\")\n",
    "# plt.ylim(10, 50)\n",
    "# plt.xlim(800, 1000)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 분류 (Classification)\n",
    "\n",
    "### Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제\n",
    "\n",
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋.\n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Feature**이미지는 28x28 크기이며 Gray scale이다.\n",
    "- **Target**은 총 10개의 class로 구성되어 있으며 각 class의 class 이름은 다음과 같다.\n",
    "\n",
    "| 레이블 | 클래스       |\n",
    "|--------|--------------|\n",
    "| 0      | T-shirt/top |\n",
    "| 1      | Trousers    |\n",
    "| 2      | Pullover    |\n",
    "| 3      | Dress       |\n",
    "| 4      | Coat        |\n",
    "| 5      | Sandal      |\n",
    "| 6      | Shirt       |\n",
    "| 7      | Sneaker     |\n",
    "| 8      | Bag         |\n",
    "| 9      | Ankle boot  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "-   **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "-   위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "-   Feature\n",
    "    -   종양에 대한 다양한 측정값들\n",
    "-   Target의 class\n",
    "    -   0 - malignant(악성종양)\n",
    "    -   1 - benign(양성종양)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 유형별 구현 정리\n",
    "\n",
    "## 공통\n",
    "\n",
    "-   Input layer(첫번째 Layer)의 in_features\n",
    "    -   입력데이터의 feature(속성) 개수에 맞춰준다.\n",
    "-   Hidden layer 수\n",
    "    -   경험적(art)으로 정한다.\n",
    "    -   Hidden layer에 Linear를 사용하는 경우 보통 feature 수를 줄여 나간다. (핵심특성들을 추출해나가는 과정의 개념.)\n",
    "\n",
    "## 회귀 모델\n",
    "\n",
    "-   output layer의 출력 unit개수(out_features)\n",
    "    -   정답의 개수\n",
    "    -   ex\n",
    "        -   집값: 1\n",
    "        -   아파트가격, 단독가격, 빌라가격: 3 => y의 개수에 맞춘다.\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   일반적으로 **None**\n",
    "    -   값의 범위가 설정되 있고 그 범위의 값을 출력하는 함수가 있을 경우\n",
    "        -   ex) 0 ~ 1: logistic(Sigmoid), -1 ~ 1: hyperbolic tangent(Tanh)\n",
    "-   loss함수\n",
    "    -   MSELoss\n",
    "-   평가지표\n",
    "    -   MSE, RMSE, R square($R^2$)\n",
    "\n",
    "## 다중분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   정답 class(고유값)의 개수\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Softmax: 클래스별 확률을 출력\n",
    "-   loss함수\n",
    "    -   **categrocial crossentropy**\n",
    "    -   파이토치 함수\n",
    "        -   **CrossEntropyLoss** = NLLLoss(정답) + LogSoftmax(모델 예측값)\n",
    "        -   **NLLLoss**\n",
    "            -   정답을 OneHot Encoding 처리 후 Loss를 계산한다.\n",
    "            -   입력으로 LogSoftmax 처리한 모델 예측값과 onehot encoding 안 된 정답을 받는다.\n",
    "        -   **LogSoftmax**\n",
    "            -   입력값에 Softmax 계산후 그 Log를 계산한다.\n",
    "                -   NLLLoss의 모델 예측값 입력값으로 처리할 때 사용한다.\n",
    "\n",
    "```python\n",
    "pred = model(input)\n",
    "loss1 = nn.NLLLoss(nn.LogSoftmax(dim=-1)(pred), y)\n",
    "# or\n",
    "loss2 = nn.CrossEntropyLoss()(pred, y)\n",
    "```\n",
    "\n",
    "## 이진분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   1개 (positive일 확률)\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Sigmoid(Logistic)\n",
    "-   loss 함수\n",
    "    -   **Binary crossentropy**\n",
    "    -   파이토치 함수: **BCELoss**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
