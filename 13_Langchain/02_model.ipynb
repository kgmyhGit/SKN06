{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain은 다양한 LLM 모델을 지원한다.\n",
    "\n",
    "- 대규모 언어 모델(LLM) 개발 회사들은 사용자들이 자신의 애플리케이션에서 LLM 모델을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "- 각 LLM은 API를 호출 library를 제공하고 있다. 그런데 개발자 입장에서는 같은 작업을 하는데 LLM에 따라 다른 코드를 사용해야하는 어려움이 있다.\n",
    "- Langchain은  다양한 LLM의 API 호출할 수 있도록 지원한다.\n",
    "    - 여러 LLM 사용을 같은 interface를 사용해 호출 할 수있게 하여 특정 모델에 종속되지 않으며,  필요에 따라 쉽게 교체할 수 있다. \n",
    "    - Langchain 지원 LLM 모델: https://python.langchain.com/docs/integrations/chat/#featured-providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치\n",
    "```bash\n",
    "pip install langchain langchain_core langchain-community  -qU\n",
    "pip install python-dotenv -qU \n",
    "pip install ipywidgets -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_core langchain-community  -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv -qU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com\n",
    "  \n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "\n",
    "2. Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 1. 로그인 -> 2. Dashboard -> 3. API Keys -> 4. Create New Secreat Key\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 환경변수를 로드한다.\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://openai.com/api/pricing/\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다.\n",
    "-  2024/11 현재 **gpt-4o-mini** 가 성능 대비 가장 저렴하다. (100만 토큰당 $0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install openai -qU\n",
    "pip install langchain-openai -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai #langchain-openai 설치시 같이 설치됨.\n",
    "from dotenv import load_dotenv # API 키들을 환경 변수로 등록.\n",
    "\n",
    "load_dotenv()  \n",
    "# working directory의 `.env` 에 작성된 환경변수들을 읽어서 os 환경변수로 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Playdata\\\\miniconda3\\\\envs\\\\langchain;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\bin;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\envs\\\\langchain\\\\Scripts;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\envs\\\\langchain\\\\bin;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\condabin;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\WINDOWS\\\\System32\\\\OpenSSH;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Bandizip;C:\\\\Program Files\\\\Graphviz\\\\bin;C:\\\\Users\\\\Playdata\\\\miniconda3;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\Library\\\\bin;C:\\\\Users\\\\Playdata\\\\miniconda3\\\\Scripts;C:\\\\Users\\\\Playdata\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\Playdata\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\Playdata\\\\Documents\\\\jdk-21.0.5\\\\bin;.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] # 환경변수 조회\n",
    "os.getenv('Path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Lib를 이용해서 GPT-4o-mini 모델에 질의\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI() # 환경변수에 API 키가 등록안 되있으면 직접 넣어서 생성.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", # 사용할 모델 종류 선택\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":\"OpenAI의 LLM 모델에 대해 설명해줘.\"}\n",
    "    ]\n",
    ")\n",
    "# {\"role\":\"채팅 주체\", \"content\":\"LLM에 전달할 내용을 text로 작성.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AZvLDV8iVqSKyRrwMm29Lc4Gni2PZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI의 LLM(대규모 언어 모델)은 자연어 처리(NLP) 작업을 수행하기 위해 설계된 인공지능 모델입니다. 이러한 모델들은 방대한 양의 텍스트 데이터를 학습하여 인간의 언어를 이해하고 생성하는 능력을 갖추고 있습니다. 여기에는 GPT(Generative Pre-trained Transformer) 시리즈가 포함되며, 이들 모델은 다음과 같은 특징을 가지고 있습니다.\\n\\n1. **Transformer 아키텍처**: LLM은 일반적으로 Transformer 아키텍처를 기반으로 하며, 이는 셀프 어텐션 메커니즘을 통해 단어 간의 관계를 효과적으로 모델링할 수 있게 해줍니다.\\n\\n2. **사전 학습 및 미세 조정**: LLM은 학습 과정에서 먼저 대량의 데이터로 사전 학습을 수행한 후, 특정 작업에 맞게 미세 조정됩니다. 이 단계에서 모델은 특정 도메인이나 작업에 필요한 추가 정보로 트레이닝을 받습니다.\\n\\n3. **다양한 응용 프로그램**: OpenAI의 LLM은 텍스트 생성, 질문 응답, 번역, 요약, 감정 분석 등 다양한 작업에 활용될 수 있습니다. 이는 매우 유연하고 범용적인 특성 덕분입니다.\\n\\n4. **상호작용성**: 이 모델들은 자연어로 사용자가 입력한 질문이나 명령에 대해 응답할 수 있으며, 대화형 AI 시스템에서도 널리 사용됩니다.\\n\\n5. **훈련 데이터의 다양성**: LLM은 다양한 출처의 텍스트 데이터를 사용하여 훈련되기 때문에, 광범위한 주제에 대한 지식을 내재하고 있습니다. 그러나 모델은 최신 정보나 전문적인 지식에 대해 제한적일 수 있습니다.\\n\\n6. **윤리적 고려사항**: OpenAI는 LLM의 사용에 있어 윤리적 문제를 고려하고 있으며, 모델의 남용을 방지하고 사회에 긍정적인 영향을 미치기 위한 여러 가지 가이드라인과 안전 장치를 마련하고 있습니다.\\n\\n이러한 특성 덕분에 OpenAI의 LLM은 여러 산업 및 애플리케이션에서 혁신적인 도구로 자리잡고 있습니다.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733125263, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=480, prompt_tokens=19, total_tokens=499, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대형 언어 모델)은 자연어 처리(NLP) 분야에서 활용되는 인공지능 모델로, 대량의 텍스트 데이터를 기반으로 학습하여 인간과 유사한 방식으로 언어를 이해하고 생성할 수 있습니다. 이러한 모델은 다음과 같은 특징을 가지고 있습니다:\n",
      "\n",
      "1. **아키텍처**: 대부분의 OpenAI LLM은 변환기(Transformer) 아키텍처를 기반으로 합니다. 이 구조는 문맥을 이해하고 단어 간의 관계를 파악하는 데 매우 효과적입니다.\n",
      "\n",
      "2. **사전 학습과 미세 조정**: LLM은 대량의 텍스트 데이터를 통해 사전 학습(pre-training)된 후, 특정 작업에 맞게 미세 조정(fine-tuning)될 수 있습니다. 이렇게 함으로써 다양한 응용 분야(예: 번역, 요약, 질문 응답 등)에 적합하게 조정할 수 있습니다.\n",
      "\n",
      "3. **비지도 학습**: LLM은 비지도 방식으로 학습합니다. 즉, 레이블이 없는 데이터에서 패턴을 학습하고, 이를 통해 언어의 구조와 의미를 파악합니다.\n",
      "\n",
      "4. **다양한 응용**: LLM은 텍스트 생성, 대화형 AI, 코드 작성, 콘텐츠 생성 등 다양한 분야에서 활용되고 있습니다. 이를 통해 보다 자연스럽고 일관된 대화형 시스템을 만들 수 있습니다.\n",
      "\n",
      "5. **제한점**: LLM은 정보의 정확성, 편향성, 그리고 문화적 맥락에 대한 이해가 부족할 수 있습니다. 또한, 모델의 크기가 크기 때문에, 처리 시간과 자원 소모가 크다는 단점이 있습니다.\n",
      "\n",
      "OpenAI의 최신 모델들은 성능과 능력이 지속적으로 향상되고 있으며, 이러한 발전은 연구자들과 개발자들이 보다 정교한 AI 시스템을 구축하는 데 기여하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.\n",
    "    - Default 로 gpt-3.5-turbo 사용\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message\n",
    "- **OpenAI**\n",
    "    - 문장 완성 모델. (text completion) model\n",
    "    - Default로 gpt-3.5-turbo-instruct 사용\n",
    "      - instruct 모델만 사용가능\n",
    "    - llm전달 입력과 llm 응답 출력 타입: str\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정한다. \n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.\n",
    "        - API key가 환경변수에 설정 되있으면 생략한다. \n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환한다.\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다. \n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공한다. 이것을 이용하면 특정 작업에 적합한 입력값을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model_name = \"gpt-4o-mini\"\n",
    "model = ChatOpenAI(\n",
    "    model=model_name\n",
    ")\n",
    "res = model.invoke(\"안녕하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-0e64ae23-d219-4c2a-9914-b0c5c5991d82-0', usage_metadata={'input_tokens': 10, 'output_tokens': 10, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(res))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 어떻게 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    temperature=0, # 무작위성을 0으로 지정.\n",
    "    max_tokens=100,# 응답 토큰수에 제한.\n",
    ")\n",
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들에 이름을 태양에서 가까운 순서대로 알려줘. \n",
    "\n",
    "[답변형식]\n",
    "- 한국어 이름(영어 이름): 행성에 대한 간단한 설명\n",
    "\"\"\"\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성, 금성, 지구, 화성, 목성, 토성, 천왕성, 해왕성\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "\n",
      "- 금성(Venus): 지구와 비슷한 크기를 가진 행성이지만, 두꺼운 이산화탄소 대기로 인해 극심한 온실 효과가 발생합니다.\n",
      "\n",
      "- 지구(Earth): 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건을 갖추고 있습니다.\n",
      "\n",
      "- 화성(Mars): '붉은 행성'으로 알려져 있으며, 과거에 물이 존재했을 가능성이 있는 행성입니다.\n",
      "\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 행성으로, 강력한 자기장과 많은 위성을 가지고 있습니다.\n",
      "\n",
      "- 토성(Saturn): 아름다운 고리로 유명한 행성으로, 가스 행성 중 하나입니다.\n",
      "\n",
      "- 천왕성(Uranus): 독특한 축으로 회전하는 행성으로, 푸른색의 대기와 많은 고리 시스템을 가지고 있습니다.\n",
      "\n",
      "- 해왕성(Neptune): 태양계에서 가장 먼 행성으로, 강한 바람과 대규모 폭풍이 특징입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(\"백합을 주제로 헤비메탈 음악의 가사를 작성해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = model.invoke(\"백합을 주제로 헤비메탈 음악의 가사를 작성해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론이죠! 백합을 주제로 한 헤비메탈 음악의 가사를 작성해볼게요. 어두운 분위기와 강렬한 이미지를 담아보았습니다.\n",
      "\n",
      "---\n",
      "\n",
      "**제목: Lament of the Lily**\n",
      "\n",
      "(Verse 1)  \n",
      "어둠 속에서 피어난,  \n",
      "하얀 인연의 백합,  \n",
      "파괴된 땅 위에,  \n",
      "어둠을 뚫고 솟아나네.\n",
      "\n",
      "(Pre-Chorus)  \n",
      "그대의 아름다움은,  \n",
      "칠흑 같은 밤을 관통해,  \n",
      "희망의 빛을 찾아,  \n",
      "우린 다시 일어설 거야.\n",
      "\n",
      "(Chorus)  \n",
      "백합의 저주, 피어나는 슬픔,  \n",
      "검은 심장에서 울려 퍼지는 노래,  \n",
      "가시와 잎사귀, 허공을 가르는 비명,  \n",
      "우린 끊임없는 전투 속에서 살아남아.\n",
      "\n",
      "(Verse 2)  \n",
      "비바람이 몰아쳐도,  \n",
      "꿋꿋이 서 있는 모습,  \n",
      "신의 손길은 잊혀진,  \n",
      "모든 걸 삼켜버린 세상.\n",
      "\n",
      "(Pre-Chorus)  \n",
      "백합의 향기 속에,  \n",
      "숨겨진 고통의 기억,  \n",
      "모든 것이 멀어져도,  \n",
      "그대의 이름을 부르리.\n",
      "\n",
      "(Chorus)  \n",
      "백합의 저주, 피어나는 슬픔,  \n",
      "검은 심장에서 울려 퍼지는 노래,  \n",
      "가시와 잎사귀, 허공을 가르는 비명,  \n",
      "우린 끊임없는 전투 속에서 살아남아.\n",
      "\n",
      "(Bridge)  \n",
      "빛을 잃은 꿈들,  \n",
      "그 속에서 다시 태어나,  \n",
      "네가 꿈꾸던 세상은,  \n",
      "여기서 끝나지 않아.\n",
      "\n",
      "(Breakdown)  \n",
      "우린 하나 되어,  \n",
      "기억의 뼈를 세우고,  \n",
      "죽음마저 삼켜내며,  \n",
      "영혼의 집을 찾아.\n",
      "\n",
      "(Chorus)  \n",
      "백합의 저주, 피어나는 슬픔,  \n",
      "검은 심장에서 울려 퍼지는 노래,  \n",
      "가시와 잎사귀, 허공을 가르는 비명,  \n",
      "우린 끊임없는 전투 속에서 살아남아.\n",
      "\n",
      "(Outro)  \n",
      "백합의 노래,  \n",
      "끝없는 전쟁의 서사,  \n",
      "어두운 밤을 지나,  \n",
      "희망의 빛 속으로 나아간다.\n",
      "\n",
      "---\n",
      "\n",
      "이런 스타일의 가사는 헤비메탈의 강렬한 분위기를 잘 살릴 수 있을 것 같아요! 추가적인 요청이나 수정 사항이 있다면 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "print(res2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI의 API 사용 가격확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks  import get_openai_callback\n",
    "with get_openai_callback() as usage:\n",
    "    # with 문 안에서 openai 모델에 요청을 하면 사용 비용이 usage에 저장됨.\n",
    "    res1 = model.invoke(\"소주 제조법을 알려주세요.\")\n",
    "    res2 = model.invoke(\"막걸리 제조법을 알려주세요.\")\n",
    "    res3 = model.invoke(\"안 취하게 술마시는 법을 알려줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 1594\n",
       "\tPrompt Tokens: 50\n",
       "\tCompletion Tokens: 1544\n",
       "Successful Requests: 3\n",
       "Total Cost (USD): $0.0009338999999999999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009338999999999999"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage.total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage.prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1544"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소주는 전통적인 한국의 증류주로, 주로 쌀, 보리, 밀 등의 곡물과 물을 사용하여 제조됩니다. 기본적인 소주 제조 과정을 아래와 같이 설명드리겠습니다.\n",
      "\n",
      "### 소주 제조법\n",
      "\n",
      "1. **원료 준비**:\n",
      "   - 주 재료로 쌀, 보리, 밀 등을 준비합니다. 이 외에도 감자나 고구마를 사용할 수도 있습니다.\n",
      "\n",
      "2. **세척 및 불리기**:\n",
      "   - 곡물을 깨끗이 세척한 후 물에 불려서 불리는 과정을 거칩니다. 이 과정은 곡물이 잘 익도록 도와줍니다.\n",
      "\n",
      "3. **증기 찌기**:\n",
      "   - 불린 곡물을 찜통에 넣고 증기로 쪄줍니다. 이 과정에서 곡물이 부풀고 익게 됩니다.\n",
      "\n",
      "4. **발효**:\n",
      "   - 찐 곡물을 식힌 후, 누룩(제빵 효모 및 발효균 혼합물)을 섞어 발효시킵니다. 이 과정에서 당분이 알코올로 변환됩니다. 일반적으로 2주에서 4주 정도 발효시킵니다.\n",
      "\n",
      "5. **증류**:\n",
      "   - 발효가 완료되면, 발효된 혼합물을 증류합니다. 이 과정에서 알코올을 분리하여 높은 도수의 소주를 얻습니다.\n",
      "\n",
      "6. **숙성**:\n",
      "   - 증류된 소주는 일정 기간 숙성됩니다. 이 과정에서 맛과 향이 더욱 깊어집니다.\n",
      "\n",
      "7. **희석 및 병입**:\n",
      "   - 숙성된 소주를 원하는 도수에 맞게 물로 희석한 후, 병에 담아 밀봉합니다.\n",
      "\n",
      "### 주의사항\n",
      "- 소주 제조는 알코올을 포함하고 있기 때문에 법적인 규제가 있을 수 있으므로, 개인이 소주를 제조하는 것은 금지된 경우가 많습니다. 반드시 관련 법규를 확인하시고, 상업적인 제조는 허가를 받아야 합니다.\n",
      "\n",
      "이상으로 소주 제조법에 대한 간단한 설명을 마칩니다!\n"
     ]
    }
   ],
   "source": [
    "print(res1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "막걸리는 한국의 전통 발효주로, 쌀과 물, 누룩을 주재료로 사용하여 만듭니다. 다음은 기본적인 막걸리 제조법입니다.\n",
      "\n",
      "### 재료\n",
      "- 쌀: 1kg (찹쌀 또는 일반 쌀)\n",
      "- 물: 2~3L (조절 가능)\n",
      "- 누룩: 100g (막걸리용 누룩)\n",
      "- 설탕: 필요에 따라 (선택 사항)\n",
      "\n",
      "### 제조 과정\n",
      "\n",
      "1. **쌀 씻기 및 불리기**:\n",
      "   - 쌀을 깨끗이 씻은 후, 물에 4~6시간 정도 불립니다. 이 과정은 쌀이 충분히 수분을 흡수할 수 있도록 도와줍니다.\n",
      "\n",
      "2. **쌀 찌기**:\n",
      "   - 불린 쌀을 체에 밭쳐 물기를 제거한 후, 찜통에 넣고 30분에서 1시간 정도 찌어줍니다. 쌀이 고루 익도록 중간에 한 번 뒤집어 주는 것이 좋습니다.\n",
      "\n",
      "3. **식히기**:\n",
      "   - 찐 쌀을 넓은 그릇이나 쟁반에 펼쳐서 식힙니다. 온도가 약 30도 정도로 내려가도록 합니다.\n",
      "\n",
      "4. **누룩 준비**:\n",
      "   - 누룩을 잘게 부수어 준비합니다. 이때 누룩이 고루 섞일 수 있도록 잘 저어줍니다.\n",
      "\n",
      "5. **혼합**:\n",
      "   - 식힌 쌀에 누룩을 넣고 잘 섞어줍니다.\n",
      "\n",
      "6. **발효**:\n",
      "   - 혼합한 쌀과 누룩을 깨끗한 발효 용기에 담고, 물을 부어줍니다. 물의 양은 쌀과 누룩의 양에 따라 조절합니다. 일반적으로는 2~3L 정도가 적당합니다.\n",
      "   - 용기의 뚜껑을 덮되, 완전히 밀폐하지 말고 약간의 공기가 통할 수 있도록 해줍니다. 이 상태에서 7일 정도 발효시킵니다. 발효 과정에서 온도는 20도에서 25도 정도가 적당합니다.\n",
      "\n",
      "7. **여과**:\n",
      "   - 발효가 끝난 후, 내용물을 체나 면포를 이용해 걸러줍니다. 이때, 액체와 고형물을 분리합니다.\n",
      "\n",
      "8. **저장**:\n",
      "   - 걸러낸 막걸리는 병에 담아 냉장 보관합니다. 추가적으로 설탕을 넣어 단맛을 조절할 수 있습니다. \n",
      "\n",
      "이제 막걸리가 완성되었습니다! 차갑게 해서 즐기시면 더욱 맛있습니다. 발효 시간을 조절하여 취향에 맞게 맛을 조절할 수 있으니 여러 번 시도해 보시길 추천합니다.\n"
     ]
    }
   ],
   "source": [
    "print(res2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안 취하게 술을 마시려면 몇 가지 방법을 고려할 수 있습니다. 하지만 가장 중요한 것은 음주를 할 때 자신의 한계를 잘 알고, 건강을 우선시하는 것입니다. 다음은 안 취하게 술 마시는 데 도움이 될 수 있는 몇 가지 팁입니다:\n",
      "\n",
      "1. **천천히 마시기**: 술을 천천히 마시면 체내에서 알코올이 분해되는 시간을 주게 되어 취하는 정도를 줄일 수 있습니다.\n",
      "\n",
      "2. **음식과 함께 마시기**: 술을 마시는 동안 음식을 함께 섭취하면 알코올의 흡수를 늦출 수 있습니다. 특히 단백질과 지방이 포함된 음식이 좋습니다.\n",
      "\n",
      "3. **물과 함께 마시기**: 술과 함께 물을 많이 마시면 탈수 현상을 예방하고, 알코올 농도를 낮출 수 있습니다. 술 한 잔에 물 한 잔을 마시는 것이 좋습니다.\n",
      "\n",
      "4. **알콜 도수 낮은 술 선택하기**: 도수가 낮은 술을 선택하면 취하는 정도를 줄일 수 있습니다. 예를 들어, 맥주나 와인처럼 도수가 낮은 음료를 선택하는 것이 좋습니다.\n",
      "\n",
      "5. **자신의 한계 인식하기**: 자신의 한계를 알고, 그 이상으로 음주하지 않도록 주의해야 합니다. 만약 이미 기분이 좋다고 느끼면 그만 마시는 것이 좋습니다.\n",
      "\n",
      "6. **간헐적으로 일어나는 활동**: 술을 마시는 중간중간에 일어나서 움직이거나 대화하는 등의 활동을 통해 신체를 활성화시키면 취하는 것을 조금 더 줄일 수 있습니다.\n",
      "\n",
      "7. **미리 계획 세우기**: 술을 마시기 전 미리 자신이 마실 양을 정하고, 그 계획에 따라 마시는 것이 좋습니다.\n",
      "\n",
      "음주를 할 때는 항상 자신의 건강과 안전을 최우선으로 생각하고, 필요할 경우 음주를 피하는 것이 가장 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "## HuggingFaceEndpoint 를 이용해 API 호출\n",
    "- Inference API 를 지원하고 10GB 이하 크기의 모델은 HuggingFaceEndpoint 를 이용해 호출해서 사용할 수있다. \n",
    "- local에 다운 받지 않아도 되고 GPU가 없을 경우 local 보다 빠르게 실행할 수 있다.\n",
    "- Inference API 지원 여부 확인\n",
    "    - HuggingFace Hub의 개별 모델 페이지에서 \"Deploy - Inference API\" 탭을 확인한다.\n",
    "### API Key (Access Token) 생성\n",
    "![huggingface_create_apikey.png](figures/huggingface_accesstoken.png)\n",
    "![huggingface_create_apikey.png](figures/huggingface_accesstoken2.png)\n",
    "- 1. 로그인 -> 2. Profile -> 3. Access Tokens 선택\n",
    "- 생성할 때 `write` 권한을 선택한다.\n",
    "- `HUGGINGFACE_API_KEY=KEY값` 을 환경변수에 등록한다.\n",
    "\n",
    "\n",
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install transformers -qU\n",
    "pip install langchain-huggingface -qU\n",
    "pip install  huggingface_hub -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-huggingface -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  huggingface_hub -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "# 파인튜닝한 모델을  huggingface-hub에 올리거나  End Point API 사용할 때 \n",
    "#     발급 받은 Access token으로 로그인해야함.\n",
    "huggingface_apikey = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "# huggingface_apikey\n",
    "login(huggingface_apikey)\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1bbbda899f4b7eafbe908e11083ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\miniconda3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-1.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8022d13ce1704598bad19fdc604c1008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41830a80e585497cb4cf3ab9afb28293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183b0cfeda8642378aa66ff516640b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1ce3d0e7cb4a8ea25fc04be2e8ed65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb9ed4dad4d46e09b17fd346c460c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f79321e95a1441099acedc491b3ddf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############# Local 모델을 사용\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\", \n",
    "    pipeline_kwargs={\"max_new_tokens\":50}  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(\"한국의 수도는 어디인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 어디인가요? 한국은 대한민국이라고 부르기도 합니다. 한국의 수도는 서울입니다. 서울은 한반도를 지나가는 동해와 서해를 잇는 도시로, 대한민국의 정치, 경제, 문'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://www.anthropic.com/pricing#anthropic-api\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "3. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "![anthropic_apikey2.png](figures/anthropic_apikey2.png)\n",
    "\n",
    "4. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "5. 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing -> complete setup\n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "```bash\n",
    "pip install anthropic -qU\n",
    "pip install -qU langchain-anthropic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic library 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"claude-3-5-haiku-latest\" # \"앤토로픽 api 결제할때 물어보는 것 체크한 내용.png\"\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"LLM에 대한 전문가로서 설명을 부탁해. 단 모르는 것은 모른다고 해. 답변은 한국어로 해줘.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic의 대표적인 LLM 모델은 Claude입니다. 현재 Claude와 Claude 2가 존재하며, 주요 특징은 다음과 같습니다:\n",
      "\n",
      "1. Claude\n",
      "- 윤리적이고 안전한 대화에 중점\n",
      "- 긴 문맥을 잘 이해하는 능력\n",
      "- 정직성과 투명성을 강조\n",
      "\n",
      "2. Claude 2\n",
      "- Claude보다 더 발전된 성능\n",
      "- 더 넓은 지식 범위\n",
      "- 더 복잡한 작업 수행 능력 향상\n",
      "- 코딩, 분석, 창의적 작업 등에서 뛰어난 성능\n",
      "\n",
      "Anthropic은 AI 안전성과 윤리에 특히 중점을 두고 모델을 개발하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# print(message)\n",
    "# print(message.content)\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# model = \"claude-3-5-haiku-latest\"\n",
    "model = \"claude-3-5-sonnet-latest\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic의 주요 LLM 모델은 Claude입니다. 현재 Claude 2와 Claude Instant, 두 가지 버전이 있습니다.\n",
      "\n",
      "Claude 2:\n",
      "- Anthropic의 최신 플래그십 모델\n",
      "- 더 긴 컨텍스트 윈도우(약 100K 토큰)\n",
      "- 더 복잡한 분석과 추론이 가능\n",
      "- 코딩, 수학, 작문 등 다양한 작업 수행\n",
      "- 더 정확하고 신뢰할 수 있는 응답 제공\n",
      "\n",
      "Claude Instant:\n",
      "- 경량화된 빠른 버전\n",
      "- 더 짧은 응답시간\n",
      "- 기본적인 대화와 간단한 작업에 적합\n",
      "- 비용 효율적\n",
      "\n",
      "두 모델 모두 윤리적 AI 원칙을 준수하도록 설계되었으며, 안전하고 유용한 방식으로 사용자를 돕는 것을 목표로 합니다.\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install ollama`\n",
    "  - `pip install langchain-ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.2.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.20 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-ollama) (0.3.21)\n",
      "Collecting ollama<1,>=0.3.0 (from langchain-ollama)\n",
      "  Downloading ollama-0.4.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (2.10.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain-ollama) (4.12.2)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from ollama<1,>=0.3.0->langchain-ollama)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (3.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.20->langchain-ollama) (2.2.3)\n",
      "Downloading langchain_ollama-0.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading ollama-0.4.2-py3-none-any.whl (13 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: httpx, ollama, langchain-ollama\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.0\n",
      "    Uninstalling httpx-0.28.0:\n",
      "      Successfully uninstalled httpx-0.28.0\n",
      "Successfully installed httpx-0.27.2 langchain-ollama-0.2.1 ollama-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama library 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "# 모델은 사전에 pull 되있어야 한다.\n",
    "model_id = \"qwen2:0.5b\"\n",
    "res = chat(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":\"한국의 수도는 어디인가요?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ollama._types.ChatResponse'>\n",
      "model='qwen2:0.5b' created_at='2024-12-03T02:41:09.6169197Z' done=True done_reason='stop' total_duration=747711900 load_duration=590027900 prompt_eval_count=18 prompt_eval_duration=54000000 eval_count=9 eval_duration=102000000 message=Message(role='assistant', content='한국의 수도는 서울입니다.', images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(type(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국의 수도는 서울입니다.\n",
      "한국의 수도는 서울입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.message.content) \n",
    "print(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=model_id, \n",
    ")\n",
    "res = model.invoke(\"미국의 수도는 어디인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='미국의 수도는 런어드입니다.', additional_kwargs={}, response_metadata={'model': 'qwen2:0.5b', 'created_at': '2024-12-03T02:45:26.5953151Z', 'done': True, 'done_reason': 'stop', 'total_duration': 247778100, 'load_duration': 10668100, 'prompt_eval_count': 18, 'prompt_eval_duration': 76000000, 'eval_count': 13, 'eval_duration': 160000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-1b339c94-31e2-4e95-bd28-559b84e2cacb-0', usage_metadata={'input_tokens': 18, 'output_tokens': 13, 'total_tokens': 31})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국의 수도는 런어드입니다.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
