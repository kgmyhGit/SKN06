{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Split\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser,StrOutputParser\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from ragas import EvaluationDataset, RunConfig, evaluate\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, Faithfulness, LLMContextPrecisionWithReference, AnswerRelevancy\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from textwrap import dedent\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"olympic_info\"\n",
    "DOC_PATH = 'data/olympic.txt'\n",
    "#  Vector DB와 동일하게 split(위의 것을 사용해도 된다.)\n",
    "loader = TextLoader(DOC_PATH, encoding='utf-8')\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4o-mini\", chunk_size=500, chunk_overlap=100\n",
    ")\n",
    "docs = loader.load_and_split(splitter) # list[Document]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 평가 데이터로 사용할 context 추출\n",
    "total_samples = 3\n",
    "\n",
    "# index shuffle 후 total_samples만큼 context 추출\n",
    "\n",
    "idx_list = list(range(len(docs)))\n",
    "random.shuffle(idx_list)\n",
    "\n",
    "eval_context_list = []\n",
    "while len(eval_context_list) < total_samples:\n",
    "    idx = idx_list.pop()\n",
    "    context = docs[idx].page_content\n",
    "    if len(context) > 100: # 100글자 이상인 text만 사용\n",
    "        eval_context_list.append(context)\n",
    "\n",
    "len(eval_context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input: 질문\n",
    "# ####### retrieved_contexts: 검색된 문서의 내용(page_content)들\n",
    "# qa_context: 질문 답변 쌍을 만들 때 참고한 context\n",
    "        # retrieved_contexts: 검색된 문서의 내용은 실제 RAG 실행시 넣는다.\n",
    "        # response: 모델의 답변 - 실제 RAG 실행시 넣는다.\n",
    "# reference: 정답\n",
    "class EvalDatasetSchema(BaseModel):\n",
    "    user_input: str = Field(..., title=\"질문(question)\")\n",
    "    qa_context: list[str] = Field(..., title=\"질문-답변 쌍을 만들 때 참조한 context.\")\n",
    "    reference: str = Field(..., title=\"질문의 정답(ground truth)\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=EvalDatasetSchema)\n",
    "\n",
    "eval_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=dedent(\"\"\"\n",
    "        당신은 RAG 평가를 위해 질문과 정답 쌍을 생성하는 인공지능 비서입니다.\n",
    "        다음 [Context] 에 문서가 주어지면 해당 문서를 기반으로 {num_questions}개 질문-정답 쌍을 생성하세요. \n",
    "        \n",
    "        질문과 정답을 생성한 후 아래의 출력 형식 GUIDE 에 맞게 생성합니다.\n",
    "        질문은 반드시 [Context] 문서에 있는 정보를 바탕으로 생성해야 합니다. [Context]에 없는 내용을 가지고 질문-정답을 절대 만들면 안됩니다.\n",
    "        질문은 간결하게 작성합니다.\n",
    "        하나의 질문에는 한 가지씩만 내용만 작성합니다. \n",
    "        질문을 만들 때 \"제공된 문맥에서\", \"문서에 설명된 대로\", \"주어진 문서에 따라\" 또는 이와 유사한 말을 하지 마세요.\n",
    "        정답은 반드시 [Context]에 있는 정보를 바탕으로 작성합니다. 없는 내용을 추가하지 않습니다.\n",
    "        질문과 정답을 만들고 그 내용이 [Context] 에 있는 항목인지 다시 한번 확인합니다.\n",
    "        생성된 질문-답변 쌍은 반드시 dictionary 형태로 정의하고 list로 묶어서 반환해야 합니다.\n",
    "        질문-답변 쌍은 반드시 {num_questions}개를 만들어야 합니다.\n",
    "\n",
    "        출력 형식: {format_instructions}\n",
    "                    \n",
    "        [Context]\n",
    "        {context}\n",
    "        \"\"\"\n",
    "    ),\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "# print(prompt_template.template)\n",
    "\n",
    "eval_dataset_generator = prompt_template | eval_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# eval_context_list 모두로 만들기\n",
    "# \n",
    "# 생성된 질문-답변을 눈으로 보고 검증한 및 수정해야 한다.\n",
    "############################################################\n",
    "eval_data_list = []\n",
    "num_questions = 5\n",
    "for context in eval_context_list:\n",
    "    _eval_data_list = eval_dataset_generator.invoke({\"context\":context, \"num_questions\":num_questions})\n",
    "    eval_data_list.extend(_eval_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_df = pd.DataFrame(eval_data_list)\n",
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 생성 된 질문/답 쌍 확인\n",
    "eval_df.head()\n",
    "eval_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store 연결\n",
    "COLLECTION_NAME = \"olympic_info\"\n",
    "PERSIST_DIRECTORY = \"vector_store/olympic_info\"\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=PERSIST_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 구성\n",
    "# prompt template. langchain hub에 등록된 것을 가져와서 사용.\n",
    "prompt_template = hub.pull(\"rlm/rag-prompt\")\n",
    "# prompt_template\n",
    "\n",
    "# Retriever 생성\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k':3,\n",
    "        'fetch_k':10,\n",
    "        'lambda_mult':0.5\n",
    "    }\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def format_docs(src_docs:dict[str, list[Document]]) -> str:\n",
    "    \"\"\"list[Document]: Vector Store에서 검색한 context들에서 \n",
    "    page_content만 추출해서 하나의 문자열로 합쳐서 반환\"\"\"\n",
    "    docs = src_docs['context']\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def str_from_documents(docs: list[Document]) -> list[str]:\n",
    "    \"\"\"list[Document]에서 page_content 값들만 추출한 list를 반환.\"\"\"\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough() # rag chain을 RunnableSequence로 만들기 위해 Runnable인 것으로 시작.\n",
    "    | {\n",
    "        \"context\": retriever, \"question\":RunnablePassthrough()\n",
    "    } # retriver -> {\"context\":list[Document], \"question\":\"user input\"}\n",
    "    | {\n",
    "        # 앞에서 넘어온 dictionary에서 context(List[Document])를 추출 -> page_content값들을 list로 반환. list[str]\n",
    "        \"source_context\" : itemgetter(\"context\") | RunnableLambda(str_from_documents), \n",
    "        \"llm_answer\": {\n",
    "            # {\"context\":list[Document]} -> str(page_content들만 모은 string)\n",
    "            \"context\": RunnableLambda(format_docs), \"question\":itemgetter(\"question\")\n",
    "        } | prompt_template | model | StrOutputParser()  # LLM 응답 처리 chain. \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_chain에 평가 질문을 입력해서 context들과 모델답변을 응답 받아 eval_dataset(eval_df)에 추가.\n",
    "context_list = []\n",
    "response_list = []\n",
    "\n",
    "for user_input in eval_df['user_input']:\n",
    "    res = rag_chain.invoke(user_input)\n",
    "    context_list.append(res['source_context'])\n",
    "    response_list.append(res['llm_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(context_list), len(response_list))\n",
    "# pprint(context_list[:2])\n",
    "# pprint(response_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"retrieved_contexts\"] = context_list # context 추가\n",
    "eval_df[\"response\"] = response_list   # 정답 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe으로 부터 EvalDataset 생성\n",
    "eval_dataset = EvaluationDataset.from_pandas(eval_df)\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"gpt-4o\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "model = ChatOpenAI(model=model_name)\n",
    "eval_llm = LangchainLLMWrapper(model)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "eval_embedding = LangchainEmbeddingsWrapper(embedding_model)\n",
    "\n",
    "\n",
    "## GPT-4o-mini 모델을 사용하여 평가 \n",
    "metrics = [\n",
    "    LLMContextRecall(llm=eval_llm),\n",
    "    LLMContextPrecisionWithReference(llm=eval_llm),\n",
    "    Faithfulness(llm=eval_llm),\n",
    "    AnswerRelevancy(llm=eval_llm, embeddings=eval_embedding)\n",
    "]\n",
    "result = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4o : 분당 토큰 리미트에 걸려 RateLimitError 가 발생할 수있다. gpt-40: 30,000 TPM, gpt-4o-mini: 200,000 TPM\n",
    "# 또한 network 연결등 문제가 발생하면 timeout 이 되어 평가가 실패할 수 있다.\n",
    "# LLM 연결과 관련해 timetout이나 ratelimiterror 발생시 metrics를 나눠서 실행, 설정 변경을 통해 해결한다.\n",
    "## https://platform.openai.com/settings/organization/limits\n",
    "run_config = RunConfig(\n",
    "    timeout=360,     # LLM 호출 이후 최대 대기 시간. 지정한 초까지 응답을 기다린다. \n",
    "    max_retries=20, # API 호출시 지정한 횟수만큼 재시도 한다.\n",
    "    max_wait=360,   # 재시도 대기 시간(초) 180초 기다린 후 재시도 한다.\n",
    "    max_workers=1   # 병렬처리 worker 수. 1로 설정하면 순차적으로 처리한다. (default: 16)\n",
    ")\n",
    "metrics1 = [\n",
    "    LLMContextRecall(llm=eval_llm),\n",
    "]\n",
    "metrics2 = [\n",
    "    LLMContextPrecisionWithReference(llm=eval_llm),\n",
    "]\n",
    "metrics3 = [\n",
    "    Faithfulness(llm=eval_llm),\n",
    "]\n",
    "metrics4 = [\n",
    "    AnswerRelevancy(llm=eval_llm, embeddings=eval_embedding)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = evaluate(dataset=eval_dataset, metrics=metrics1, run_config=run_config)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = evaluate(dataset=eval_dataset, metrics=metrics2, run_config=run_config)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = evaluate(dataset=eval_dataset, metrics=metrics3, run_config=run_config)\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = evaluate(dataset=eval_dataset, metrics=metrics4, run_config=run_config)\n",
    "result4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
