{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef06541a-3111-46cb-a3be-4b50d8267e1a",
   "metadata": {},
   "source": [
    "# Transformers 를 이용해 Backbone 사용\n",
    "\n",
    "## Transformers 설치\n",
    "- `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817d6ef-7239-4738-89c1-1d6a64d8a8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3d4825a-fe9f-4d1e-b9e0-7bb78745b840",
   "metadata": {},
   "source": [
    "### Tokenizer, Model Loading\n",
    "- Huggingface 모델 허브에서 제공하는 처리 모델을 다운받아 로딩한다.\n",
    "- 다운로드된 모델은 `사용자 home 디렉토리\\.cache\\huggingface` 에 저장된다.\n",
    "- 미리 학습된 언어 모델을 다운받아 사용할 때는 그 언어모델이 사용한 tokenizer를 같이 받아서 사용한다.\n",
    "\n",
    "### [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- Huggingface 에서 제공하는 다양한 모델들은 손쉽게 불러오고 사용할 수 있도록 설계된 유틸리티 클래스들을 말한다.\n",
    "- 미리 학습된 특정 모델의 이름(모델 허브상에서 제공되는 이름)이나 저장된 local 경로를 제공하면 해당 모델에 맞는 적절한 클래스와 구성 요소를 자동으로 로드한다.\n",
    "- 사용자는 모델을 사용하기 위한 정확한 클래스를 몰라도 쉽게 다양한 종류의 모델을 사용할 수있다.\n",
    "\n",
    "#### 주요 Auto Class\n",
    "- 기본 모델 Loading\n",
    "    1. **AutoModel**\n",
    "       - 주어진 모델 이름에 맞는 사전 학습된 모델을 자동으로 로드한다.\n",
    "       - 예: `AutoModel.from_pretrained(\"bert-base-uncased\")`: BERT 모델을 로드한다.\n",
    "    2. **AutoTokenizer**\n",
    "       - 해당 모델에 적합한 토크나이저를 자동으로 로드한다.\n",
    "       - 예: `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: BERT 모델에 맞는 토크나이저를 로드한다.\n",
    "    3. **AutoConfig**\n",
    "       - 모델의 설정(config)을 자동으로 로드한다. 모델 설정에는 모델의 하이퍼파라미터와 모델 구조 정보가 포함된다. 이 설정을 이용해 모델을 생성할 수있다.\n",
    "       - 예: `AutoConfig.from_pretrained(\"bert-base-uncased\")`\n",
    "- Task 처리 모델 Loading\n",
    "    - Pretrained backbone 모델에 각 task 에 맞는 estimator layer를 추가한 모델을 생성해 제공한다.\n",
    "    - 주요 모델들\n",
    "        1. **AutoModelForSequenceClassification**\n",
    "           - 시퀀스(Text) 분류 작업을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")`\n",
    "        2. **AutoModelForQuestionAnswering**\n",
    "           - 질문-응답 작업을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")`\n",
    "        3. **AutoModelForTokenClassification**\n",
    "           - 토큰 분류 작업(예: 개체명 인식)을 위한 모델을 자동으로 로드한다.\n",
    "           - 예: `AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74aacd2-703b-4c20-9c90-14fddc725fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada489a8-039a-432c-bc8c-3e95cebfa4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n"
     ]
    }
   ],
   "source": [
    "model_id = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(type(tokenizer)) # pretrained 된 토크나저저\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "print(type(model))# pretrained 된 모델\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "print(type(config)) # model_id의 모델 설정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2b9e88-ee5e-4de3-a311-31498aa7c837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 1045, 2572, 1037, 2879, 1012,  102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]])}\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "text = \"I am a boy.\"\n",
    "tokens = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",  \n",
    "    # 결과의 타입.(pt: torch.tensor, tf: tensorflow.tensor, np: ndaray, default: list)\n",
    ")\n",
    "pprint(tokens)\n",
    "print(type(tokens['attention_mask']))\n",
    "# attention_mask: 각 토큰이 실제토큰인지 (1), padding(0) 인지 구분하는 값들로 구성.\n",
    "# input_ids: 토큰 id\n",
    "# token_type_ids:  입력으로 두개 문장을 받았을때 몇번째 문장의 토큰인지를 구분. (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e7f1471-43a1-478f-aad8-7c51db4f6651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2572, 1037, 2879, 1012, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)  # 토큰 id만 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fc44778-9246-410a-87dc-efdd94fe8c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 0],\n",
      "                    [1, 1, 1, 1, 1, 1, 0],\n",
      "                    [1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 1045, 2572, 1037, 2879, 102, 0],\n",
      "               [101, 1045, 2572, 7501, 1012, 102, 0],\n",
      "               [101, 1045, 2572, 2183, 2000, 2188, 102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "## 여러 문서(문장)들을 한번에 토큰화\n",
    "### -> max_length, padding, truncation 설정\n",
    "### -> default 토큰 길이: 개별 문장별 길이에 맞춰 토큰생성.\n",
    "text_list = [\"I am a boy\", \"I am hungry.\", \"I am going to home.\"]\n",
    "token_list = tokenizer(\n",
    "    text_list, \n",
    "    max_length=7, # 최대 토큰 수\n",
    "    padding=True, # padding 추가\n",
    "    truncation=True # max_length가 넘을 경우 짤라낸다.\n",
    ")\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd02bf0f-3c85-4945-bf9a-53f511adbd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "                    [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 1045, 2572, 1037, 2879, 102, 0, 0],\n",
      "               [101, 1045, 2572, 7501, 1012, 102, 0, 0],\n",
      "               [101, 1045, 2572, 2183, 2000, 2188, 1012, 102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "token_list = tokenizer(\n",
    "    text_list, \n",
    "    # max_length=7, # 최대 토큰 수\n",
    "    padding=True, # max_length 설정 없이 padding=True => 제일 긴 토큰을 가진 문서에 맞춘다.\n",
    "    # truncation=True # max_length가 넘을 경우 짤라낸다.\n",
    ")\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4c20236-ac02-41e4-921b-8b6711b990d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 어휘개수: 30522 30522\n",
      "token_id -> token 단어: [PAD]\n",
      "token_id -> token 단어: ['[UNK]', 'spring', 'despite']\n",
      "token_단어-> token_id: 3500\n",
      "token_단어-> token_id: [3500, 1045, 2572]\n"
     ]
    }
   ],
   "source": [
    "## 토크나이저의 정보들 조회\n",
    "print(\"총 어휘개수:\", tokenizer.vocab_size, len(tokenizer))\n",
    "print(\"token_id -> token 단어:\", tokenizer.convert_ids_to_tokens(0))\n",
    "print(\"token_id -> token 단어:\", tokenizer.convert_ids_to_tokens([100, 3500, 2750]))\n",
    "print('token_단어-> token_id:', tokenizer.convert_tokens_to_ids('spring'))\n",
    "print('token_단어-> token_id:', tokenizer.convert_tokens_to_ids(['spring', \"i\", 'am']))\n",
    "# tokenizer.get_vocab() # 모든 토큰들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea7503-520e-4327-b2f3-8b3f63e38b23",
   "metadata": {},
   "source": [
    "### tokenizer에 토큰들을 추가\n",
    "- tokenizer와 모델을 같이 받았고, tokenizer에 토큰들을 추가했을 경우 이것을 모델에 적용시켜야 한다.\n",
    "- 모델의 embedding vector의 크기를 재조정해야 하기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52db779f-a684-486c-811c-41a52e54df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30532\n",
      "30532\n"
     ]
    }
   ],
   "source": [
    "# 추가\n",
    "print(len(tokenizer))\n",
    "tokenizer.add_tokens([\"미ㅏㅇ러ㅣㅏ\", \"ㅌ티치\", \"마이ㅣ치\", \"키이ㅏㅣ딩\", \"이ㅏ미칭\"])\n",
    "# 사이즈\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5988bbc4-a906-4ca4-bf8d-37ecda8cb15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30532, 768, padding_idx=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 업데이트\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef773cb-9947-4b0b-9deb-16bb7ea1b61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5a02d44-7271-4db4-b8c0-7c14037dce3a",
   "metadata": {},
   "source": [
    "## kcbert\n",
    "- BERT 모델을 한글 텍스트로 학습 시킨 Pretrained model.\n",
    "    - BERT는 Transformer의 Encoder 부분을 이용해 구현된 언어모델\n",
    "    - https://arxiv.org/abs/1810.04805 \n",
    "- https://huggingface.co/beomi/kcbert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af872e-410e-4a3c-8ec3-2b1e1a680ad6",
   "metadata": {},
   "source": [
    "### 토크나이저 모델 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bac27579-6db6-4d08-b757-7067df3d5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_id = \"beomi/kcbert-base\"  # base model => Feature Extractor\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1304b288-04ee-4c5e-b0d6-5d8a3b3d9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"안녕하세요. 반갑습니다.\", \n",
    "             \"kcbert는 bert 모델을 한국어로 학습한 모델입니다.\", \n",
    "             \"토크나이저와 모델은 같은 ID의 것으로 받아야 합니다.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33baa96b-d6ff-4b50-98ef-7db93b99a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = tokenizer(\n",
    "    text_list, \n",
    "    max_length=10,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2518bce-6fd0-4512-9d59-0def0653095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "print(type(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28171f1e-7588-4583-b027-224ab6e09c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[    2, 19017,  8482,    17,  1483,  4981,  8046,    17,     3,     0],\n",
      "        [    2,    76,  4773,  4545, 13146,  4401,  4008,    67, 13146,     3],\n",
      "        [    2,  3160,  4147, 16991,  4488,  4196, 16505,  4057,  8066,     3]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52498095-7fb8-4da1-9103-d22b10e9ef43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19acd0fe-5952-4ee6-97ee-e4fcf41ec093",
   "metadata": {},
   "source": [
    "### BERT 모델을 이용해 context vector 추출\n",
    "#### Model 추론결과\n",
    "- **last_hidden_state**\n",
    "    - 모든 token들에 대한 feature\n",
    "    - 출력이 **many**인 작업에 사용한다.\n",
    "- **pooler_output**\n",
    "    - 입력 문장, 텍스트에 대한 context vector 이다.\n",
    "    - 이 값은 **문장을 입력받아 처리하는 task**(ex: 문서분류-감정분석,문장카테고리분류, 문장유사도 분석)의 입력으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42e0659e-22b0-47b0-96e6-037d9befad8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(300, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3da19e46-0c61-485b-acce-a7cd3ab4f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(attention_mask=A, input_ids=I, token_type_ids=TI)\n",
    "outputs = model(**token_list) # token_list: dict -> 가변인자에 item별로 나눠서 입력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b72c694-99ca-4418-b6aa-57f81d682f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a60421ac-d856-47c5-aa68-22452a13e7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cba0cfb6-1791-43b9-bd98-904411268cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['last_hidden_state'].shape\n",
    "# [3: 문장수-batch, 10: 토큰수 - seq-len, 768: 개별토큰의 feature개수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42876c17-803d-4a01-a413-66a7699bf3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pooler_output'].shape\n",
    "# 문서(문장) 별로 하나씩 -> 문서의 context vector (문서에 대한 feature vector)의 역할."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2104ec-0875-4db3-9fb7-44ab391785a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
